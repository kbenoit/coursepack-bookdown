<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>MY451 Introduction to Quantitative Analysis</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="This course is intended for those with little or no past training in quantitative methods. The course is an intensive introduction to some of the principles and methods of statistical analysis in social research. Topics covered in MY451 include descriptive statistics, basic ideas of inference and estimation, contingency tables and an introduction to linear regression models.">
  <meta name="generator" content="bookdown 0.1.9 and GitBook 2.6.7">

  <meta property="og:title" content="MY451 Introduction to Quantitative Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This course is intended for those with little or no past training in quantitative methods. The course is an intensive introduction to some of the principles and methods of statistical analysis in social research. Topics covered in MY451 include descriptive statistics, basic ideas of inference and estimation, contingency tables and an introduction to linear regression models." />
  <meta name="github-repo" content="kbenoit/coursepack-bookdown" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="MY451 Introduction to Quantitative Analysis" />
  
  <meta name="twitter:description" content="This course is intended for those with little or no past training in quantitative methods. The course is an intensive introduction to some of the principles and methods of statistical analysis in social research. Topics covered in MY451 include descriptive statistics, basic ideas of inference and estimation, contingency tables and an introduction to linear regression models." />
  

<meta name="author" content="Jouni Kuha">
<meta name="author" content="Department of Methodology">
<meta name="author" content="London School of Economics and Political Science">


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="c-means.html">
<link rel="next" href="c-3waytables.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />










<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MY451 Introduction to Quantitative Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Course information</a></li>
<li class="chapter" data-level="1" data-path="c-intro.html"><a href="c-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="c-intro.html"><a href="c-intro.html#s-intro-purpose"><i class="fa fa-check"></i><b>1.1</b> What is the purpose of this course?</a></li>
<li class="chapter" data-level="1.2" data-path="c-intro.html"><a href="c-intro.html#s-intro-definitions"><i class="fa fa-check"></i><b>1.2</b> Some basic definitions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-subj"><i class="fa fa-check"></i><b>1.2.1</b> Subjects and variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-vartypes"><i class="fa fa-check"></i><b>1.2.2</b> Types of variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-descr"><i class="fa fa-check"></i><b>1.2.3</b> Description and inference</a></li>
<li class="chapter" data-level="1.2.4" data-path="c-intro.html"><a href="c-intro.html#ss-intro-def-assoc"><i class="fa fa-check"></i><b>1.2.4</b> Association and causation</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="c-intro.html"><a href="c-intro.html#s-intro-outline"><i class="fa fa-check"></i><b>1.3</b> Outline of the course</a></li>
<li class="chapter" data-level="1.4" data-path="c-intro.html"><a href="c-intro.html#s-intro-maths"><i class="fa fa-check"></i><b>1.4</b> The use of mathematics and computing</a><ul>
<li class="chapter" data-level="1.4.1" data-path="c-intro.html"><a href="c-intro.html#symbolic-mathematics-and-mathematical-notation"><i class="fa fa-check"></i><b>1.4.1</b> Symbolic mathematics and mathematical notation</a></li>
<li class="chapter" data-level="1.4.2" data-path="c-intro.html"><a href="c-intro.html#computing-1"><i class="fa fa-check"></i><b>1.4.2</b> Computing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c-descr1.html"><a href="c-descr1.html"><i class="fa fa-check"></i><b>2</b> Descriptive statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-intro"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-examples"><i class="fa fa-check"></i><b>2.2</b> Example data sets</a></li>
<li class="chapter" data-level="2.3" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-1cat"><i class="fa fa-check"></i><b>2.3</b> Single categorical variable</a><ul>
<li class="chapter" data-level="2.3.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-distr"><i class="fa fa-check"></i><b>2.3.1</b> Describing the sample distribution</a></li>
<li class="chapter" data-level="2.3.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-tables"><i class="fa fa-check"></i><b>2.3.2</b> Tabular methods: Tables of frequencies</a></li>
<li class="chapter" data-level="2.3.3" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-charts"><i class="fa fa-check"></i><b>2.3.3</b> Graphical methods: Bar charts</a></li>
<li class="chapter" data-level="2.3.4" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cat-descriptives"><i class="fa fa-check"></i><b>2.3.4</b> Simple descriptive statistics</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-2cat"><i class="fa fa-check"></i><b>2.4</b> Two categorical variables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-tables"><i class="fa fa-check"></i><b>2.4.1</b> Two-way contingency tables</a></li>
<li class="chapter" data-level="2.4.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-cond"><i class="fa fa-check"></i><b>2.4.2</b> Conditional proportions</a></li>
<li class="chapter" data-level="2.4.3" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-assoc"><i class="fa fa-check"></i><b>2.4.3</b> Conditional distributions and associations</a></li>
<li class="chapter" data-level="2.4.4" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-descr"><i class="fa fa-check"></i><b>2.4.4</b> Describing an association using conditional proportions</a></li>
<li class="chapter" data-level="2.4.5" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-2cat-gamma"><i class="fa fa-check"></i><b>2.4.5</b> A measure of association for ordinal variables</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-1cont"><i class="fa fa-check"></i><b>2.5</b> Sample distributions of a single continuous variable</a><ul>
<li class="chapter" data-level="2.5.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cont-tab"><i class="fa fa-check"></i><b>2.5.1</b> Tabular methods</a></li>
<li class="chapter" data-level="2.5.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-1cont-graphs"><i class="fa fa-check"></i><b>2.5.2</b> Graphical methods</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-nums"><i class="fa fa-check"></i><b>2.6</b> Numerical descriptive statistics</a><ul>
<li class="chapter" data-level="2.6.1" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-nums-central"><i class="fa fa-check"></i><b>2.6.1</b> Measures of central tendency</a></li>
<li class="chapter" data-level="2.6.2" data-path="c-descr1.html"><a href="c-descr1.html#ss-descr1-nums-variation"><i class="fa fa-check"></i><b>2.6.2</b> Measures of variation</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-2cont"><i class="fa fa-check"></i><b>2.7</b> Associations which involve continuous variables</a></li>
<li class="chapter" data-level="2.8" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-presentation"><i class="fa fa-check"></i><b>2.8</b> Presentation of tables and graphs</a></li>
<li class="chapter" data-level="2.9" data-path="c-descr1.html"><a href="c-descr1.html#s-descr1-app"><i class="fa fa-check"></i><b>2.9</b> Appendix: Country data</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c-samples.html"><a href="c-samples.html"><i class="fa fa-check"></i><b>3</b> Samples and populations</a><ul>
<li class="chapter" data-level="3.1" data-path="c-samples.html"><a href="c-samples.html#s-samples-intro"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="c-samples.html"><a href="c-samples.html#s-samples-finpops"><i class="fa fa-check"></i><b>3.2</b> Finite populations</a></li>
<li class="chapter" data-level="3.3" data-path="c-samples.html"><a href="c-samples.html#s-samples-samples"><i class="fa fa-check"></i><b>3.3</b> Samples from finite populations</a></li>
<li class="chapter" data-level="3.4" data-path="c-samples.html"><a href="c-samples.html#s-samples-infpops"><i class="fa fa-check"></i><b>3.4</b> Conceptual and infinite populations</a></li>
<li class="chapter" data-level="3.5" data-path="c-samples.html"><a href="c-samples.html#s-samples-popdistrs"><i class="fa fa-check"></i><b>3.5</b> Population distributions</a></li>
<li class="chapter" data-level="3.6" data-path="c-samples.html"><a href="c-samples.html#s-samples-inference"><i class="fa fa-check"></i><b>3.6</b> Need for statistical inference</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c-tables.html"><a href="c-tables.html"><i class="fa fa-check"></i><b>4</b> Statistical inference for two-way tables</a><ul>
<li class="chapter" data-level="4.1" data-path="c-tables.html"><a href="c-tables.html#s-tables-intro"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="c-tables.html"><a href="c-tables.html#s-tables-tests"><i class="fa fa-check"></i><b>4.2</b> Significance tests</a></li>
<li class="chapter" data-level="4.3" data-path="c-tables.html"><a href="c-tables.html#s-tables-chi2test"><i class="fa fa-check"></i><b>4.3</b> The chi-square test of independence</a><ul>
<li class="chapter" data-level="4.3.1" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-null"><i class="fa fa-check"></i><b>4.3.1</b> Hypotheses</a></li>
<li class="chapter" data-level="4.3.2" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-ass"><i class="fa fa-check"></i><b>4.3.2</b> Assumptions of a significance test</a></li>
<li class="chapter" data-level="4.3.3" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-stat"><i class="fa fa-check"></i><b>4.3.3</b> The test statistic</a></li>
<li class="chapter" data-level="4.3.4" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-sdist"><i class="fa fa-check"></i><b>4.3.4</b> The sampling distribution of the test statistic</a></li>
<li class="chapter" data-level="4.3.5" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-Pval"><i class="fa fa-check"></i><b>4.3.5</b> The P-value</a></li>
<li class="chapter" data-level="4.3.6" data-path="c-tables.html"><a href="c-tables.html#ss-tables-chi2test-conclusions"><i class="fa fa-check"></i><b>4.3.6</b> Drawing conclusions from a test</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="c-tables.html"><a href="c-tables.html#s-tables-summary"><i class="fa fa-check"></i><b>4.4</b> Summary of the chi-square test of independence</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="c-probs.html"><a href="c-probs.html"><i class="fa fa-check"></i><b>5</b> Inference for population proportions</a><ul>
<li class="chapter" data-level="5.1" data-path="c-probs.html"><a href="c-probs.html#s-probs-intro"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="c-probs.html"><a href="c-probs.html#s-probs-examples"><i class="fa fa-check"></i><b>5.2</b> Examples</a></li>
<li class="chapter" data-level="5.3" data-path="c-probs.html"><a href="c-probs.html#s-probs-distribution"><i class="fa fa-check"></i><b>5.3</b> Probability distribution of a dichotomous variable</a></li>
<li class="chapter" data-level="5.4" data-path="c-probs.html"><a href="c-probs.html#s-probs-pointest"><i class="fa fa-check"></i><b>5.4</b> Point estimation of a population probability</a></li>
<li class="chapter" data-level="5.5" data-path="c-probs.html"><a href="c-probs.html#s-probs-test1sample"><i class="fa fa-check"></i><b>5.5</b> Significance test of a single proportion</a><ul>
<li class="chapter" data-level="5.5.1" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-hypotheses"><i class="fa fa-check"></i><b>5.5.1</b> Null and alternative hypotheses</a></li>
<li class="chapter" data-level="5.5.2" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-teststatistic"><i class="fa fa-check"></i><b>5.5.2</b> The test statistic</a></li>
<li class="chapter" data-level="5.5.3" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-samplingd"><i class="fa fa-check"></i><b>5.5.3</b> The sampling distribution of the test statistic and P-values</a></li>
<li class="chapter" data-level="5.5.4" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-conclusions"><i class="fa fa-check"></i><b>5.5.4</b> Conclusions from the test</a></li>
<li class="chapter" data-level="5.5.5" data-path="c-probs.html"><a href="c-probs.html#ss-probs-test1sample-summary"><i class="fa fa-check"></i><b>5.5.5</b> Summary of the test</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci"><i class="fa fa-check"></i><b>5.6</b> Confidence interval for a single proportion</a><ul>
<li class="chapter" data-level="5.6.1" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci-intro"><i class="fa fa-check"></i><b>5.6.1</b> Introduction</a></li>
<li class="chapter" data-level="5.6.2" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci-calc"><i class="fa fa-check"></i><b>5.6.2</b> Calculation of the interval</a></li>
<li class="chapter" data-level="5.6.3" data-path="c-probs.html"><a href="c-probs.html#s-probs-1sampleci-int"><i class="fa fa-check"></i><b>5.6.3</b> Interpretation of confidence intervals</a></li>
<li class="chapter" data-level="5.6.4" data-path="c-probs.html"><a href="c-probs.html#ss-means-ci-vstests"><i class="fa fa-check"></i><b>5.6.4</b> Confidence intervals vs. significance tests</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="c-probs.html"><a href="c-probs.html#s-probs-2samples"><i class="fa fa-check"></i><b>5.7</b> Inference for comparing two proportions</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="c-contd.html"><a href="c-contd.html"><i class="fa fa-check"></i><b>6</b> Continuous variables: Population and sampling distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="c-contd.html"><a href="c-contd.html#s-contd-intro"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="c-contd.html"><a href="c-contd.html#s-contd-popdistrs"><i class="fa fa-check"></i><b>6.2</b> Population distributions of continuous variables</a><ul>
<li class="chapter" data-level="6.2.1" data-path="c-contd.html"><a href="c-contd.html#ss-contd-popdistrs-params"><i class="fa fa-check"></i><b>6.2.1</b> Population parameters and their point estimates</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="c-contd.html"><a href="c-contd.html#s-contd-probdistrs"><i class="fa fa-check"></i><b>6.3</b> Probability distributions of continuous variables</a><ul>
<li class="chapter" data-level="6.3.1" data-path="c-contd.html"><a href="c-contd.html#ss-contd-probdistrs-general"><i class="fa fa-check"></i><b>6.3.1</b> General comments</a></li>
<li class="chapter" data-level="6.3.2" data-path="c-contd.html"><a href="c-contd.html#ss-contd-probdistrs-normal"><i class="fa fa-check"></i><b>6.3.2</b> The normal distribution as a population distribution</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="c-contd.html"><a href="c-contd.html#s-contd-clt"><i class="fa fa-check"></i><b>6.4</b> The normal distribution as a sampling distribution</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="c-means.html"><a href="c-means.html"><i class="fa fa-check"></i><b>7</b> Analysis of population means</a><ul>
<li class="chapter" data-level="7.1" data-path="c-means.html"><a href="c-means.html#s-means-intro"><i class="fa fa-check"></i><b>7.1</b> Introduction and examples</a></li>
<li class="chapter" data-level="7.2" data-path="c-means.html"><a href="c-means.html#s-means-descr"><i class="fa fa-check"></i><b>7.2</b> Descriptive statistics for comparisons of groups</a><ul>
<li class="chapter" data-level="7.2.1" data-path="c-means.html"><a href="c-means.html#ss-means-descr-graphs"><i class="fa fa-check"></i><b>7.2.1</b> Graphical methods of comparing sample distributions</a></li>
<li class="chapter" data-level="7.2.2" data-path="c-means.html"><a href="c-means.html#ss-means-descr-tables"><i class="fa fa-check"></i><b>7.2.2</b> Comparing summary statistics</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="c-means.html"><a href="c-means.html#s-means-inference"><i class="fa fa-check"></i><b>7.3</b> Inference for two means from independent samples</a><ul>
<li class="chapter" data-level="7.3.1" data-path="c-means.html"><a href="c-means.html#ss-means-inference-intro"><i class="fa fa-check"></i><b>7.3.1</b> Aims of the analysis</a></li>
<li class="chapter" data-level="7.3.2" data-path="c-means.html"><a href="c-means.html#ss-means-inference-test"><i class="fa fa-check"></i><b>7.3.2</b> Significance testing: The two-sample t-test</a></li>
<li class="chapter" data-level="7.3.3" data-path="c-means.html"><a href="c-means.html#ss-means-inference-ci"><i class="fa fa-check"></i><b>7.3.3</b> Confidence intervals for a difference of two means</a></li>
<li class="chapter" data-level="7.3.4" data-path="c-means.html"><a href="c-means.html#ss-means-inference-variants"><i class="fa fa-check"></i><b>7.3.4</b> Variants of the test and confidence interval</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="c-means.html"><a href="c-means.html#s-means-1sample"><i class="fa fa-check"></i><b>7.4</b> Tests and confidence intervals for a single mean</a></li>
<li class="chapter" data-level="7.5" data-path="c-means.html"><a href="c-means.html#s-means-dependent"><i class="fa fa-check"></i><b>7.5</b> Inference for dependent samples</a></li>
<li class="chapter" data-level="7.6" data-path="c-means.html"><a href="c-means.html#s-means-tests3"><i class="fa fa-check"></i><b>7.6</b> Further comments on significance tests</a><ul>
<li class="chapter" data-level="7.6.1" data-path="c-means.html"><a href="c-means.html#ss-means-tests3-errors"><i class="fa fa-check"></i><b>7.6.1</b> Different types of error</a></li>
<li class="chapter" data-level="7.6.2" data-path="c-means.html"><a href="c-means.html#ss-means-tests3-power"><i class="fa fa-check"></i><b>7.6.2</b> Power of significance tests</a></li>
<li class="chapter" data-level="7.6.3" data-path="c-means.html"><a href="c-means.html#ss-means-tests3-importance"><i class="fa fa-check"></i><b>7.6.3</b> Significance vs. importance</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="c-regression.html"><a href="c-regression.html"><i class="fa fa-check"></i><b>8</b> Linear regression models</a><ul>
<li class="chapter" data-level="8.1" data-path="c-regression.html"><a href="c-regression.html#s-regression-intro"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="c-regression.html"><a href="c-regression.html#s-regression-descr"><i class="fa fa-check"></i><b>8.2</b> Describing association between two continuous variables</a><ul>
<li class="chapter" data-level="8.2.1" data-path="c-regression.html"><a href="c-regression.html#ss-regression-descr-intro"><i class="fa fa-check"></i><b>8.2.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2.2" data-path="c-regression.html"><a href="c-regression.html#ss-regression-descr-plots"><i class="fa fa-check"></i><b>8.2.2</b> Graphical methods</a></li>
<li class="chapter" data-level="8.2.3" data-path="c-regression.html"><a href="c-regression.html#ss-regression-descr-assoc"><i class="fa fa-check"></i><b>8.2.3</b> Linear associations</a></li>
<li class="chapter" data-level="8.2.4" data-path="c-regression.html"><a href="c-regression.html#ss-regression-descr-corr"><i class="fa fa-check"></i><b>8.2.4</b> Measures of association: covariance and correlation</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="c-regression.html"><a href="c-regression.html#s-regression-simple"><i class="fa fa-check"></i><b>8.3</b> Simple linear regression models</a><ul>
<li class="chapter" data-level="8.3.1" data-path="c-regression.html"><a href="c-regression.html#ss-regression-simple-intro"><i class="fa fa-check"></i><b>8.3.1</b> Introduction</a></li>
<li class="chapter" data-level="8.3.2" data-path="c-regression.html"><a href="c-regression.html#ss-regression-simple-def"><i class="fa fa-check"></i><b>8.3.2</b> Definition of the model</a></li>
<li class="chapter" data-level="8.3.3" data-path="c-regression.html"><a href="c-regression.html#ss-regression-simple-int"><i class="fa fa-check"></i><b>8.3.3</b> Interpretation of the model parameters</a></li>
<li class="chapter" data-level="8.3.4" data-path="c-regression.html"><a href="c-regression.html#ss-regression-simple-est"><i class="fa fa-check"></i><b>8.3.4</b> Estimation of the parameters</a></li>
<li class="chapter" data-level="8.3.5" data-path="c-regression.html"><a href="c-regression.html#ss-regression-simple-inf"><i class="fa fa-check"></i><b>8.3.5</b> Statistical inference for the regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="c-regression.html"><a href="c-regression.html#s-regression-causality"><i class="fa fa-check"></i><b>8.4</b> Interlude: Association and causality</a></li>
<li class="chapter" data-level="8.5" data-path="c-regression.html"><a href="c-regression.html#s-regression-multiple"><i class="fa fa-check"></i><b>8.5</b> Multiple linear regression models</a><ul>
<li class="chapter" data-level="8.5.1" data-path="c-regression.html"><a href="c-regression.html#ss-regression-multiple-intro"><i class="fa fa-check"></i><b>8.5.1</b> Introduction</a></li>
<li class="chapter" data-level="8.5.2" data-path="c-regression.html"><a href="c-regression.html#ss-regression-multiple-def"><i class="fa fa-check"></i><b>8.5.2</b> Definition of the model</a></li>
<li class="chapter" data-level="8.5.3" data-path="c-regression.html"><a href="c-regression.html#ss-regression-multiple-unchanged"><i class="fa fa-check"></i><b>8.5.3</b> Unchanged elements from simple linear models</a></li>
<li class="chapter" data-level="8.5.4" data-path="c-regression.html"><a href="c-regression.html#ss-regression-multiple-beta"><i class="fa fa-check"></i><b>8.5.4</b> Interpretation and inference for the regression coefficients</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="c-regression.html"><a href="c-regression.html#s-regression-dummies"><i class="fa fa-check"></i><b>8.6</b> Including categorical explanatory variables</a><ul>
<li class="chapter" data-level="8.6.1" data-path="c-regression.html"><a href="c-regression.html#ss-regression-dummies-def"><i class="fa fa-check"></i><b>8.6.1</b> Dummy variables</a></li>
<li class="chapter" data-level="8.6.2" data-path="c-regression.html"><a href="c-regression.html#ss-regression-dummies-example"><i class="fa fa-check"></i><b>8.6.2</b> A second example</a></li>
</ul></li>
<li class="chapter" data-level="8.7" data-path="c-regression.html"><a href="c-regression.html#s-regression-rest"><i class="fa fa-check"></i><b>8.7</b> Other issues in linear regression modelling</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="c-3waytables.html"><a href="c-3waytables.html"><i class="fa fa-check"></i><b>9</b> Analysis of 3-way contingency tables</a></li>
<li class="chapter" data-level="10" data-path="c-more.html"><a href="c-more.html"><i class="fa fa-check"></i><b>10</b> More statistics…</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#computer-classes"><i class="fa fa-check"></i>Computer classes</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#general-instructions"><i class="fa fa-check"></i>General instructions</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#introduction-to-spss"><i class="fa fa-check"></i>Introduction to SPSS</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-2-class-descriptive-statistics-for-categorical-data-and-entering-data"><i class="fa fa-check"></i>WEEK 2 class: Descriptive statistics for categorical data, and entering data</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-3-class"><i class="fa fa-check"></i>WEEK 3 class</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-4-class-two-way-contingency-tables"><i class="fa fa-check"></i>WEEK 4 class: Two-way contingency tables</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-5-class-inference-for-two-population-means"><i class="fa fa-check"></i>WEEK 5 class: Inference for two population means</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-7-class-inference-for-population-proportions"><i class="fa fa-check"></i>WEEK 7 class: Inference for population proportions</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-7-class-correlation-and-simple-linear-regression-1"><i class="fa fa-check"></i>WEEK 7 class: Correlation and simple linear regression 1</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-8-class-simple-linear-regression-and-3-way-tables"><i class="fa fa-check"></i>WEEK 8 class: Simple linear regression and 3-way tables</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-9-class-multiple-linear-regression"><i class="fa fa-check"></i>WEEK 9 class: Multiple linear regression</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#week-10-class-review-and-multiple-linear-regression"><i class="fa fa-check"></i>WEEK 10 class: Review and Multiple linear regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#statistical-tables"><i class="fa fa-check"></i>Statistical tables</a><ul>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#table-of-standard-normal-tail-probabilities"><i class="fa fa-check"></i>Table of standard normal tail probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#table-of-critical-values-for-t-distributions"><i class="fa fa-check"></i>Table of critical values for t-distributions</a></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html#table-of-critical-values-for-chi-square-distributions"><i class="fa fa-check"></i>Table of critical values for chi-square distributions</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/kbenoit/coursepack-bookdown/" alt="This prototype was developed by Tobias Pester.">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MY451 Introduction to Quantitative Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="c-regression" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Linear regression models</h1>
<div id="s-regression-intro" class="section level2">
<h2><span class="header-section-number">8.1</span> Introduction</h2>
<p>This chapter continues the theme of analysing statistical associations between variables. The methods described here are appropriate when the response variable <span class="math inline">\(Y\)</span> is a continuous, interval level variable. We will begin by considering bivariate situations where the only explanatory variable <span class="math inline">\(X\)</span> is also a continuous variable. Section <a href="c-regression.html#s-regression-descr">8.2</a> first discusses graphical and numerical descriptive techniques for this case, focusing on two very commonly used tools: a <em>scatterplot</em> of two variables, and a measure of association known as the <em>correlation</em> coefficient. Section <a href="c-regression.html#s-regression-simple">8.3</a> then describes methods of statistical inference for associations between two continuous variables. This is done in the context of a statistical model known as the <em>simple linear regression model</em>.</p>
<p>The ideas of simple linear regression modelling can be extended to a much more general and powerful set methods known as <em>multiple linear regression models</em>. These can have several explanatory variables, which makes it possible to examine associations between any explanatory variable and the response variable, while controlling for other explanatory variables. An important reason for the usefulness of these models is that they play a key role in statistical analyses which correspond to research questions that are causal in nature. As an interlude, we discuss issues of causality in research design and analysis briefly in Section <a href="c-regression.html#s-regression-causality">8.4</a>. Multiple linear models are then introduced in Section <a href="c-regression.html#s-regression-multiple">8.5</a>. The models can also include categorical explanatory variables with any number of categories, as explained in Section <a href="c-regression.html#s-regression-dummies">8.6</a>.</p>
<p>The following example will be used for illustration throughout this chapter:</p>
<p><strong>Example 8.1: Indicators of Global Civil Society</strong></p>
<p>The <em>Global Civil Society 2004/5</em> yearbook gives tables of a range of characteristics of the countries of the world.<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a> The following measures will be considered in this chapter:</p>
<ul>
<li><p>Gross Domestic Product (<strong>GDP</strong>) per capita in 2001 (in current international dollars, adjusted for purchasing power parity)</p></li>
<li><p><strong>Income level</strong> of the country in three groups used by the Yearbook, as Low income, Middle income or High income</p></li>
<li><p><strong>Income inequality</strong> measured by the Gini index (with 0 representing perfect equality and 100 perfect inequality)</p></li>
<li><p>A measure of <strong>political rights and civil liberties</strong> in 2004, obtained as the average of two indices for these characteristics produced by the Freedom House organisation (1 to 7, with higher values indicating more rights and liberties)</p></li>
<li><p>World Bank Institute’s measure of control of <strong>corruption</strong> for 2002 (with high values indicating low levels of corruption)</p></li>
<li><p>Net <strong>primary school enrolment</strong> ratio 2000-01 (%)</p></li>
<li><p><strong>Infant mortality rate</strong> 2001 (% of live births)</p></li>
</ul>
<p>We will discuss various associations between these variables. It should be noted that the analyses are mainly illustrative examples, and the choices of explanatory and response variables do not imply any strong claims about causal connections between them. Also, the fact that different measures refer to slightly different years is ignored; in effect, we treat each variable as a measure of “recent” situation in the countries. The full data set used here includes 165 countries. Many of the variables are not available for all of them, so most of the analyses below use a smaller number of countries.</p>
</div>
<div id="s-regression-descr" class="section level2">
<h2><span class="header-section-number">8.2</span> Describing association between two continuous variables</h2>
<div id="ss-regression-descr-intro" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Introduction</h3>
<p>Suppose for now that we are considering data on two continuous variables. The descriptive techniques discussed in this section do not strictly speaking require a distinction between an explanatory variable and a response variable, but it is nevertheless useful in many if not most applications. We will reflect this in the notation by denoting the variables <span class="math inline">\(X\)</span> (for the explanatory variable) and <span class="math inline">\(Y\)</span> (for the response variable). The observed data consist of the pairs of observations <span class="math inline">\((X_{1}, Y_{1}), (X_{2}, Y_{2}), \dots, (X_{n}, Y_{n})\)</span> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for each of the <span class="math inline">\(n\)</span> subjects in a sample, or, with more concise notation, <span class="math inline">\((X_{i}, Y_{i})\)</span> for <span class="math inline">\(i=1,2,\dots,n\)</span>.</p>
<p>We are interested in analysing the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Methods for <em>describing</em> this association in the sample are first described in this section, initially with some standard graphical methods in Section <a href="c-regression.html#ss-regression-descr-plots">8.2.2</a>. This leads to a discussion in Section <a href="c-regression.html#ss-regression-descr-assoc">8.2.3</a> of what we actually mean by associations in this context, and then to a definion of numerical summary measures for such associations in Section <a href="c-regression.html#ss-regression-descr-corr">8.2.4</a>. Statistical <em>inference</em> for the associations will be considered in Section <a href="c-regression.html#s-regression-simple">8.3</a>.</p>
</div>
<div id="ss-regression-descr-plots" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Graphical methods</h3>
<div id="scatterplots" class="section level4 unnumbered">
<h4>Scatterplots</h4>
<p>The standard statistical graphic for summarising the association between two continuous variables is a <strong>scatterplot</strong>. An example of it is given in Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>, which shows a scatterplot of Control of corruption against GDP per capita for 61 countries for which the corruption variable is at least 60 (the motivation of this restriction will be discussed later). The two axes of the plot show possible values of the two variables. The horizontal axis, here corresponding to Control of corruption, is conventionally used for the explanatory variable <span class="math inline">\(X\)</span>, and is often referred to as the <strong>X-axis</strong>. The vertical axis, here used for GDP per capita, then corresponds to the response variable <span class="math inline">\(Y\)</span>, and is known as the <strong>Y-axis</strong>.</p>
<div class="figure"><span id="fig:f-corruption1"></span>
<img src="corruption1.png" alt="A scatterplot of Control of corruption vs. GDP per capita in the Global Civil Society data set, for 61 countries with Control of corruption at least 60. The dotted lines are drawn to the point corresponding to the United Kingdom." width="510" />
<p class="caption">Figure 8.1: A scatterplot of Control of corruption vs. GDP per capita in the Global Civil Society data set, for 61 countries with Control of corruption at least 60. The dotted lines are drawn to the point corresponding to the United Kingdom.</p>
</div>
<p>The observed data are shown as points in the scatterplot, one for each of the <span class="math inline">\(n\)</span> units. The location of each point is determined by its values of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. For example, Figure <a href="c-regression.html#fig:f-corruption1">8.1</a> highlights the observation for the United Kingdom, for which the corruption measure (<span class="math inline">\(X\)</span>) is 94.3 and GDP per capita (<span class="math inline">\(Y\)</span>) is $24160. The point for UK is thus placed at the intersection of a vertical line drawn from 94.3 on the <span class="math inline">\(X\)</span>-axis and a horizontal line from 24160 on the <span class="math inline">\(Y\)</span>-axis, as shown in the plot.</p>
<p>The principles of good graphical presentation on clear labelling, avoidance of spurious decoration and so on (c.f. Section <a href="c-descr1.html#s-descr1-presentation">2.8</a>) are the same for scatterplots as for any statistical graphics. Because the crucial visual information in a scatterplot is the shape of the cloud of the points, it is now often not necessary for the scales of the axes to begin at zero, especially if this is well outside the ranges of the observed values of the variables (as it is for the <span class="math inline">\(X\)</span>-axis of Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>). Instead, the scales are typically selected so that the points cover most of the plotting surface. This is done by statistical software, but there are many situations were it is advisable to overrule the automatic selection (e.g. for making scatterplots of the same variables in two different samples directly comparable).</p>
<p>The main purpose of a scatterplot is to examine possible associations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Loosely speaking, this means considering the shape and orientation of the cloud of points in the graph. In Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>, for example, it seems that most of the points are in a cluster sloping from lower left to upper right. This indicates that countries with low levels of Control of corruption (i.e. high levels of corruption itself) tend to have low GDP per capita, and those with little corruption tend to have high levels of GDP. A more careful discussion of such associations again relates them to the formal definition in terms of conditional distributions, and also provides a basis for the methods of inference introduced later in this chapter. We will resume the discussion of these issues in Section <a href="c-regression.html#ss-regression-descr-assoc">8.2.3</a> below. Before that, however, we will digress briefly from the main thrust of this chapter in order to describe a slightly different kind of scatterplot.</p>
</div>
<div id="line-plots-for-time-series" class="section level4 unnumbered">
<h4>Line plots for time series</h4>
<p>A very common special case of a scatterplot is one where the observations correspond to measurements of a variable for the same unit at several occasions over time. This is illustrated by the following example (another one is Figure <a href="c-descr1.html#fig:f-houseprices">2.9</a>):</p>
<p><em>Example: Changes in temperature, 1903–2004</em></p>
<p>Figure <a href="c-regression.html#fig:f-temperatures">8.2</a> summarises data on average annual temperatures over the past century in five locations. The data were obtained from the GISS Surface Temperature (GISTEMP) database maintained by the NASA Goddard Institute for Space Studies.<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a> The database contains time series of average monthly surface temperatures from several hundred meterological stations across the world. The five sites considered here are Haparanda in Northern Sweden, Independence, Kansas in the USA, Choshi on the east coast of Japan, Kimberley in South Africa, and the Base Orcadas Station on Laurie Island, off the coast of Antarctica. These were chosen rather haphazardly for this illustration, with the aim of obtaining a geographically scattered set of rural or small urban locations (to avoid issues with the heating effects of large urban areas). The temperature for each year at each location is here recorded as the difference from the temperature at that location in 1903.<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a></p>
<div class="figure"><span id="fig:f-temperatures"></span>
<img src="temperplot.png" alt="Changes of average annual temperature (11-year moving averages) from 1903 in five locations. See the text for further details. Source: The GISTEMP database &lt;data.giss.nasa.gov/gistemp/&gt;" width="491" />
<p class="caption">Figure 8.2: Changes of average annual temperature (11-year moving averages) from 1903 in five locations. See the text for further details. Source: The GISTEMP database &lt;data.giss.nasa.gov/gistemp/&gt;</p>
</div>
<p>Consider first the data for Haparanda only. Here we have two variables, year and temperature, and 102 pairs of observations of them, one for each year between 1903 and 2004. These pairs could now be plotted in a scatterplot as described above. Here, however, we can go further to enhance the visual effect of the plot. This is because the observations represent measurements of a variable (temperature difference) for the same unit (the town of Haparanda) at several successive times (years). These 102 measurements form a <em>time series</em> of temperature differences for Haparanda over 1903–2004. A standard graphical trick for such series is to connect the points for successive times by lines, making it easy for the eye to follow the changes over time in the variable on the <span class="math inline">\(Y\)</span>-axis. In Figure <a href="c-regression.html#fig:f-temperatures">8.2</a> this is done for Haparanda using a solid line. Note that doing this would make no sense for scatter plots like the one in Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>, because all the points there represent different subjects, in that case countries.</p>
<p>We can easily include several such series in the same graph. In Figure <a href="c-regression.html#fig:f-temperatures">8.2</a> this is done by plotting the temperature differences for each of the five locations using different line styles. The graph now summarises data on three variables, year, temperature and location. We can then examine changes over time for any one location, but also compare patterns of changes between them. Here there is clearly much variation within and between locations, but also some common features. Most importantly, the temperatures have all increased over the past century. In all five locations the average annual temperatures at the end of the period were around 1–2<span class="math inline">\(^{\circ}\)</span>C higher than in 1903.</p>
<p>A set of time series like this is an example of dependent data in the sense discussed in Section <a href="c-means.html#s-means-dependent">7.5</a>. There we considered cases with pairs of observations, where the two observations in each pair had to be treated as statistically dependent. Here all of the temperature measurements for one location are dependent, probably with strongest dependence between adjacent years and less dependence between ones further apart. This means that we will not be able to analyse these data with the methods described later in this chapter, because these assume statistically independent observations. Methods of statistical modelling and inference for dependent data of the kind illustrated by the temperature example are beyond the scope of this course. This, however, does not prevent us from using a plot like Figure <a href="c-regression.html#fig:f-temperatures">8.2</a> to <em>describe</em> such data.</p>
</div>
</div>
<div id="ss-regression-descr-assoc" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Linear associations</h3>
<p>Consider again statistically independent observations of <span class="math inline">\((X_{i}, Y_{i})\)</span>, such as those displayed in Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>. Recall the definition that two variables are associated if the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is different for different values of <span class="math inline">\(X\)</span>. In the two-sample examples of Chapter <a href="c-means.html#c-means">7</a> this could be examined by comparing two conditional distributions, since <span class="math inline">\(X\)</span> had only two possible values. Now, however, <span class="math inline">\(X\)</span> has many (in principle, infinitely many) possible values, so we will need to somehow define and compare conditional distributions given each of them. We will begin with a rather informal discussion of how this might be done. This will lead directly to a more precise and formal definition introduced in Section <a href="c-regression.html#s-regression-simple">8.3</a>.</p>
<div class="figure"><span id="fig:f-corruption2"></span>
<img src="corruption2.png" alt="The same scatterplot of Control of corruption vs. GDP per capita as in Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>, augmented by the best-fitting (least squares) straight line (solid line) and reference lines for two example values of Control of corruption (dotted lines)." width="510" />
<p class="caption">Figure 8.3: The same scatterplot of Control of corruption vs. GDP per capita as in Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>, augmented by the best-fitting (least squares) straight line (solid line) and reference lines for two example values of Control of corruption (dotted lines).</p>
</div>
<p>Figure <a href="c-regression.html#fig:f-corruption2">8.3</a> shows the same scatterplot as Figure <a href="c-regression.html#fig:f-corruption1">8.1</a>. Consider first one value of <span class="math inline">\(X\)</span> (Control of corruption), say 65. To get a rough idea of the conditional distribution of <span class="math inline">\(Y\)</span> (GDP per capita) given this value of <span class="math inline">\(X\)</span>, we could examine the sample distribution of the values of <span class="math inline">\(Y\)</span> for the units for which the value of <span class="math inline">\(X\)</span> is close to 65. These correspond to the points near the vertical line drawn at <span class="math inline">\(X=65\)</span> in Figure <a href="c-regression.html#fig:f-corruption2">8.3</a>. This can be repeated for any value of <span class="math inline">\(X\)</span>; for example, Figure <a href="c-regression.html#fig:f-corruption2">8.3</a> also includes a vertical reference line at <span class="math inline">\(X=95\)</span>, for examining the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=95\)</span>.<a href="#fn42" class="footnoteRef" id="fnref42"><sup>42</sup></a></p>
<p>As in Chapter <a href="c-means.html#c-means">7</a>, associations between variables will here be considered almost solely in terms of differences in the <em>means</em> of the conditional distributions of <span class="math inline">\(Y\)</span> at different values of <span class="math inline">\(X\)</span>. For example, Figure <a href="c-regression.html#fig:f-corruption2">8.3</a> suggests that the conditional mean of <span class="math inline">\(Y\)</span> when X is 65 is around or just under 10000. At <span class="math inline">\(X=95\)</span>, on the other hand, the conditional mean seems to be between 20000 and 25000. The mean of <span class="math inline">\(Y\)</span> is thus higher at the larger value of X. More generally, this finding is consistent across the scatterplot, in that the conditional mean of <span class="math inline">\(Y\)</span> appears to increase when we consider increasingly large values of <span class="math inline">\(X\)</span>, indicating that higher levels of Control of corruption are associated with higher average levels of GDP. This is often expressed by saying that the conditional mean of <span class="math inline">\(Y\)</span> increases when we “increase” <span class="math inline">\(X\)</span>.<a href="#fn43" class="footnoteRef" id="fnref43"><sup>43</sup></a> This is the sense in which we will examine associations between continuous variables: does the conditional mean of <span class="math inline">\(Y\)</span> change (increase or decrease) when we increase <span class="math inline">\(X\)</span>? If it does, the two variables are associated; if it does not, there is no association of this kind. This definition also agrees with the one linking association with prediction: if the mean of <span class="math inline">\(Y\)</span> is different for different values of <span class="math inline">\(X\)</span>, knowing the value of <span class="math inline">\(X\)</span> will clearly help us in making predictions about likely values of <span class="math inline">\(Y\)</span>. Based on the information in Figure <a href="c-regression.html#fig:f-corruption2">8.3</a>, for example, our best guesses of the GDPs of two countries would clearly be different if we were told that the control of corruption measure was 65 for one country and 95 for the other.</p>
<p>The <em>nature</em> of the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is characterised by <em>how</em> the values of <span class="math inline">\(Y\)</span> change when <span class="math inline">\(X\)</span> increases. First, it is almost always reasonable to conceive these changes as reasonably smooth and gradual. In other words, if two values of <span class="math inline">\(X\)</span> are close to each other, the conditional means of <span class="math inline">\(Y\)</span> will be similar too; for example, if the mean of <span class="math inline">\(Y\)</span> is 5 when <span class="math inline">\(X=10\)</span>, its mean when <span class="math inline">\(X=10.01\)</span> is likely to be quite close to 5 rather than, say, 405. In technical terms, this means that the conditional mean of <span class="math inline">\(Y\)</span> will be described by a smooth mathematical function of <span class="math inline">\(X\)</span>. Graphically, the means of <span class="math inline">\(Y\)</span> as <span class="math inline">\(X\)</span> increases will then trace a smooth curve in the scatterplot. The simplest possibility for such a curve is a straight line. This possibility is illustrated by plot (a) of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a> (this and the other five plots in the figure display artificial data, generated for this illustration). Here all of the points fall on a line, so that when <span class="math inline">\(X\)</span> increases, the values of <span class="math inline">\(Y\)</span> increase at a constant rate. A relationship like this is known as a <strong>linear association</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Linear associations are the starting point for examining associations between continuous variables, and often the only ones considered. In this chapter we too will focus almost completely on them.</p>
<div class="figure"><span id="fig:f-scatterplots"></span>
<img src="scatterplots.png" alt="Scatterplots of artificial data sets of two variables. Each plot also shows the best-fitting (least squares) straight line and the correlation coefficient r." width="510" />
<p class="caption">Figure 8.4: Scatterplots of artificial data sets of two variables. Each plot also shows the best-fitting (least squares) straight line and the correlation coefficient <span class="math inline">\(r\)</span>.</p>
</div>
<p>In plot (a) of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a> all the points are exactly on the straight line. This indicates a <em>perfect</em> linear association, where <span class="math inline">\(Y\)</span> can be predicted exactly if <span class="math inline">\(X\)</span> is known, so that the association is <em>deterministic</em>. Such a situation is neither realistic in practice, nor necessary for the association to be described as linear. All that is required for the latter is that the conditional <em>means</em> of <span class="math inline">\(Y\)</span> given different values of <span class="math inline">\(X\)</span> fall (approximately) on a straight line. This is illustrated by plot (b) of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a>, which shows a scatterplot of individual observations together with an approximation of the line of the means of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> (how the line was drawn will be explained later). Here the linear association is not perfect, as the individual points are not all on the same line but scattered around it. Nevertheless, the line seems to capture an important systematic feature of the data, which is that the <em>average</em> values of <span class="math inline">\(Y\)</span> increase at an approximately constant rate as <span class="math inline">\(X\)</span> increases. This combination of systematic and random elements is characteristic of all statistical associations, and it is also central to the formal setting for statistical inference for linear associations described in Section <a href="c-regression.html#s-regression-simple">8.3</a> below.</p>
<p>The <strong>direction</strong> of a linear association can be either <strong>positive</strong> or <strong>negative</strong>. Plots (a) and (b) of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a> show a positive association, because increasing <span class="math inline">\(X\)</span> is associated with increasing average values of <span class="math inline">\(Y\)</span>. This is indicated by the upward slope of the line describing the association. Plot (c) shows an example of a negative association, where the line slopes downwards and increasing values of <span class="math inline">\(X\)</span> are associated with decreasing values of <span class="math inline">\(Y\)</span>. The third possibility, illustrated by plot (d), is that the line slopes neither up nor down, so that the mean of <span class="math inline">\(Y\)</span> is the same for all values of <span class="math inline">\(X\)</span>. In this case there is no (linear) association between the variables.</p>
<p>Not all associations between continuous variables are linear, as shown by the remaining two plots of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a>. These illustrate two kinds of <strong>nonlinear</strong> associations. In plot (e), the association is still clearly <em>monotonic</em>, meaning that average values of <span class="math inline">\(Y\)</span> change in the same direction — here increase — when <span class="math inline">\(X\)</span> increases. The rate of this increase, however, is not constant, as indicated by the slightly curved shape of the cloud of points. The values of <span class="math inline">\(Y\)</span> seem to increase faster for small values of <span class="math inline">\(X\)</span> than for large ones. A straight line drawn through the scatterplot captures the general direction of the increase, but misses its nonlinearity. One practical example of such a relationship is the one between years of job experience and salary: it is often found that salary increases fastest early on in a person’s career and more slowly later on.</p>
<p>Plot (f) shows a nonlinear and nonmonotonic relationship: as <span class="math inline">\(X\)</span> increases, average values of <span class="math inline">\(Y\)</span> first decrease to a minimum, and then increase again, resulting in a U-shaped scatterplot. A straight line is clearly an entirely inadequate description of such a relationship. A nonmonotonic association of this kind might be seen, for example, when considering the dependence of the failure rates of some electrical components (<span class="math inline">\(Y\)</span>) on their age (<span class="math inline">\(X\)</span>). It might then be that the failure rates were high early (from quick failures of flawed components) and late on (from inevitable wear and tear) and lowest in between for “middle-aged but healthy” components.</p>
<div class="figure"><span id="fig:f-corruption3"></span>
<img src="corruption3.png" alt="A scatterplot of Control of corruption vs. GDP per capita for 163 countries in the Global Civil Society data set. The solid line is the best-fitting (least squares) straight line for the points." width="510" />
<p class="caption">Figure 8.5: A scatterplot of Control of corruption vs. GDP per capita for 163 countries in the Global Civil Society data set. The solid line is the best-fitting (least squares) straight line for the points.</p>
</div>
<p>Returning to real data, recall that we have so far considered control of corruption and GDP per capita only among countries with a Control of corruption score of at least 60. The scatterplot for these, shown in Figure <a href="c-regression.html#fig:f-corruption2">8.3</a>, also includes a best-fitting straight line. The observed relationship is clearly positive, and seems to be fairly well described by a straight line. For countries with relatively low levels of corruption, the association between control of corruption and GDP can be reasonably well characterised as linear.</p>
<p>Consider now the set of all countries, including also those with high levels of corruption (scores of less than 60). In a scatterplot for them, shown in Figure <a href="c-regression.html#fig:f-corruption3">8.5</a>, the points with at least 60 on the <span class="math inline">\(X\)</span>-axis are the same as those in Figure <a href="c-regression.html#fig:f-corruption2">8.3</a>, and the new points are to the left of them. The plot now shows a nonlinear relationship comparable to the one in plot (e) of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a>. The linear relationship which was a good description for the countries considered above is thus not adequate for the full set of countries. Instead, it seems that the association is much weaker for the countries with high levels of corruption, essentially all of which have fairly low values of GDP per capita. The straight line fitted to the plot identifies the overall positive association, but cannot describe its nonlinearity. This example further illustrates how scatterplots can be used to examine relationships between variables and to assess whether they can be best described as linear or nonlinear associations.<a href="#fn44" class="footnoteRef" id="fnref44"><sup>44</sup></a></p>
<p>So far we have said nothing about how the exact location and direction of the straight lines shown in the figures have been selected. These are determined so that the fitted line is in a certain sense the best possible one for describing the data in the scatterplot. Because the calculations needed for this are also (and more importantly) used in the context of statistical inference for such data, we will postpone a description of them until Section <a href="c-regression.html#ss-regression-simple-est">8.3.4</a>. For now we can treat the line simply as a visual summary of the linear association in a scatterplot.</p>
</div>
<div id="ss-regression-descr-corr" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Measures of association: covariance and correlation</h3>
<p>A scatterplot is a very powerful tool for examining sample associations of pairs of variables in detail. Sometimes, however, this is more than we really need for an initial summary of a data set, especially if there are many variables and thus many possible pairs of them. It is then convenient also to be able to summarise each pairwise association using a single-number measure of association. This section introduces the correlation coefficient, the most common such measure for continuous variables. It is a measure of the strength of <em>linear</em> associations of the kind defined above.</p>
Suppose that we consider two variables, denoted <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This again implies a distinction between an explanatory and a response variable, to maintain continuity of notation between different parts of this chapter. The correlation coefficient itself, however, is completely symmetric, so that its value for a pair of variables will be the same whether or not we treat one or the other of them as explanatory for the other. First, recall from equation of standard deviation towards the end of Section <a href="c-descr1.html#ss-descr1-nums-variation">2.6.2</a> that the sample standard deviations of the two variables are calculated as
<span class="math display">\[\begin{equation}s_{x} = \sqrt{\frac{\sum(X_{i}-\bar{X})^{2}}{n-1}}
\text{and}
s_{y} = \sqrt{\frac{\sum (Y_{i}-\bar{Y})^{2}}{n-1}}
\label{eq:sdyx}\end{equation}\]</span>
where the subscripts <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> identify the two variables, and <span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(\bar{Y}\)</span> are their sample means. A new statistic is the <strong>sample covariance</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, defined as
<span class="math display">\[\begin{equation}s_{xy} = \frac{\sum (X_{i}-\bar{X})(Y_{i}-\bar{Y})}{n-1}.
\label{eq:sxy}\end{equation}\]</span>
<p>This is a measure of linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is positive if the sample association is positive and negative if the association is negative.</p>
In theoretical statistics, covariance is the fundamental summary of sample and population associations between two continuous variables. For descriptive purposes, however, it has the inconvenient feature that its magnitude depends on the units in which <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are measured. This makes it difficult to judge whether a value of the covariance for particular variables should be regarded as large or small. To remove this complication, we can standardise the sample covariance by dividing it by the standard deviations, to obtain the statistic
<span class="math display">\[\begin{equation}r=\frac{s_{xy}}{s_{x}s_{y}} =
\frac
{
\sum (X_{i}-\bar{X})(Y_{i}-\bar{Y})
}{
\sqrt{
\sum\left(X_{i}-\bar{X}\right)^{2}
\sum\left(Y_{i}-\bar{Y}\right)^{2}}
}.
\label{eq:corr}\end{equation}\]</span>
<p>This is the (sample) <strong>correlation</strong> coefficient, or correlation for short, between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. It is also often (e.g. in SPSS) known as <em>Pearson’s</em> correlation coefficient after Karl Pearson (of the <span class="math inline">\(\chi^{2}\)</span> test, see first footnote in Chapter <a href="c-tables.html#c-tables">4</a>), although both the word and the statistic are really due to Sir Francis Galton.<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a></p>
<p>The properties of the correlation coefficient can be described by going through the same list as for the <span class="math inline">\(\gamma\)</span> coefficient in Section <a href="c-descr1.html#ss-descr1-2cat-gamma">2.4.5</a>. While doing so, it is useful to refer to the examples in Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a>, where the correlations are also shown.</p>
<ul>
<li><p><strong>Sign</strong>: Correlation is positive if the <em>linear</em> association between the variables is positive, i.e. if the best-fitting straight line slopes upwards (as in plots a, b and e) and negative if the association is negative (c). A zero correlation indicates complete lack of linear association (d and f).</p></li>
<li><p><strong>Extreme values</strong>: The largest possible correlation is <span class="math inline">\(+1\)</span> (plot a) and the smallest <span class="math inline">\(-1\)</span>, indicating perfect positive and negative linear associations respectively. More generally, the magnitude of the correlation indicates the strength of the association, so that the closer to <span class="math inline">\(+1\)</span> or <span class="math inline">\(-1\)</span> the correlation is, the stronger the association (e.g. compare plots a–d). It should again be noted that the correlation captures only the linear aspect of the association, as illustrated by the two nonlinear cases in Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a>. In plot (e), there is curvature but also a strong positive trend, and the latter is reflected in a fairly high correlation. In plot (f), the trend is absent and the correlation is 0, even though there is an obvious nonlinear relationship. Thus the correlation coefficient is a reasonable initial summary of the strength of association in (e), but completely misleading in (f).</p></li>
<li><p><strong>Formal interpretation</strong>: The correlation coefficient cannot be interpreted as a Proportional Reduction in Error (PRE) measure, but its square can. The latter statistic, so-called coefficient of determination or <span class="math inline">\(R^{2}\)</span>, is described in Section <a href="c-regression.html#ss-regression-simple-int">8.3.3</a>.</p></li>
<li><p><strong>Substantive interpretation</strong>: As with any measure of association, the question of whether a particular sample correlation is high or low is not a purely statistical question, but depends on the nature of the variables. This can be judged properly only with the help of experience of correlations between similar variables in different contexts. As one very rough rule thumb it might be said that in many social science contexts correlations greater than 0.4 (or smaller than <span class="math inline">\(-0.4\)</span>) would typically be considered noteworthy and ones greater than 0.7 quite strong.</p></li>
</ul>
<p>Returning to real data, Table <a href="c-regression.html#tab:t-civilsoc-r">8.1</a> shows the correlation coefficients for all fifteen distinct pairs of the six continuous variables in the Global Civil Society data set mentioned in Example 8.1. This is an example of a <strong>correlation matrix</strong>, which is simply a table with the variables as both its rows and columns, and the correlation between each pair of variables given at the intersection of corresponding row and column. For example, the correlation of GDP per capita and School enrolment is here 0.42. This is shown at the intersection of the first row (GDP) and fifth column (School enrolment), and also of the fifth row and first column. In general, every correlation is shown twice in the matrix, once in its upper triangle and once in the lower. The triangles are separated by a list of ones on the diagonal of the matrix. This simply indicates that the correlation of any variable with itself is 1, which is true by definition and thus of no real interest.</p>
<table>
<caption><span id="tab:t-civilsoc-r">Table 8.1: </span>Correlation matrix of six continuous variables in the Global Civil Society data set. See Example 8.1 for more information on the variables.</caption>
<thead>
<tr class="header">
<th align="right">Variable</th>
<th align="right">GDP</th>
<th align="right">Gini</th>
<th align="right">Pol. </th>
<th align="right">Corrupt. </th>
<th align="right">School</th>
<th align="right">IMR</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">GDP per capita [GDP ]</td>
<td align="right">1</td>
<td align="right">-0.39</td>
<td align="right">0.51</td>
<td align="right">0.77</td>
<td align="right">0.42</td>
<td align="right">-0.62</td>
</tr>
<tr class="even">
<td align="right">Income inequality [Gini ]</td>
<td align="right">-0.39</td>
<td align="right">1</td>
<td align="right">-0.15</td>
<td align="right">-0.27</td>
<td align="right">-0.27</td>
<td align="right">0.42</td>
</tr>
<tr class="odd">
<td align="right">Political rights [Pol. ]</td>
<td align="right">0.51</td>
<td align="right">-0.15</td>
<td align="right">1</td>
<td align="right">0.59</td>
<td align="right">0.40</td>
<td align="right">-0.44</td>
</tr>
<tr class="even">
<td align="right">Control of corruption [Corrupt. ]</td>
<td align="right">0.77</td>
<td align="right">-0.27</td>
<td align="right">0.59</td>
<td align="right">1</td>
<td align="right">0.41</td>
<td align="right">-0.64</td>
</tr>
<tr class="odd">
<td align="right">School enrolment [School ]</td>
<td align="right">0.42</td>
<td align="right">-0.27</td>
<td align="right">0.40</td>
<td align="right">0.41</td>
<td align="right">1</td>
<td align="right">-0.73</td>
</tr>
<tr class="even">
<td align="right">Infant mortality [IMR ]</td>
<td align="right">-0.62</td>
<td align="right">0.42</td>
<td align="right">-0.44</td>
<td align="right">-0.64</td>
<td align="right">-0.73</td>
<td align="right">1</td>
</tr>
</tbody>
</table>
<p>All of the observed associations in this example are in unsurprising directions. For example, School enrolment is positively correlated with GDP, Political rights and Control of corruption, and negatively correlated with Income inequality and Infant mortality. In other words, countries with large percentages of children enrolled in primary school tend to have high levels of GDP per capita and of political rights and civil liberties, and low levels of corruption, income inequality and infant mortality. The strongest associations in these data are between GDP per capita and Control of corruption (<span class="math inline">\(r=0.77\)</span>) and School enrolment and Infant mortality rate (<span class="math inline">\(r=-0.73\)</span>), and the weakest between Income inequality on the one hand and Political rights, Control of corruption and School enrolment on the other (correlations of <span class="math inline">\(-0.15\)</span>, <span class="math inline">\(-0.27\)</span> and <span class="math inline">\(-0.27\)</span> respectively).</p>
<p>These correlations describe only the linear element of sample associations, but give no hint of any nonlinear ones. For example, the correlation of 0.77 between GDP and Control of corruption summarises the way the observations cluster around the straight line shown in Figure <a href="c-regression.html#fig:f-corruption3">8.5</a>. The correlation is high because this increase in GDP as Control of corruption increases is quite strong, but it gives no indication of the nonlinearity of the association. A scatterplot is needed for revealing this feature of the data. The correlation for the restricted set of countries shown in Figure <a href="c-regression.html#fig:f-corruption2">8.3</a> is 0.82.</p>
<p>A correlation coefficient can also be defined for the joint population distribution of two variables. The sample correlation <span class="math inline">\(r\)</span> can then be treated as an estimate of the population correlation, which is often denoted by <span class="math inline">\(\rho\)</span> (the lower-case Greek “rho”). Statistical inference for the population correlation can also be derived. For example, SPSS automatically outputs significance tests for the null hypothesis that <span class="math inline">\(\rho\)</span> is 0, i.e. that there is no linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in the population. Here, however, we will not discuss this, choosing to treat <span class="math inline">\(r\)</span> purely as a descriptive sample statistic. The next section provides a different set of tools for inference on population associations.</p>
</div>
</div>
<div id="s-regression-simple" class="section level2">
<h2><span class="header-section-number">8.3</span> Simple linear regression models</h2>
<div id="ss-regression-simple-intro" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Introduction</h3>
<p>The rest of this course is devoted to the method of linear regression modelling. Its purpose is the analysis of associations in cases where the response variable is a continuous, interval level variable, and the possibly several explanatory variables can be of any type. We begin in this section with <em>simple</em> linear regression, where there is only one explanatory variable. We will further assume that this is also continuous. The situation considered here is thus the same as in the previous section, but here the focus will be on statistical inference rather than description. Most of the main concepts of linear regression can be introduced in this context. Those that go beyond it are described in subsequent sections. Section <a href="c-regression.html#s-regression-multiple">8.5</a> introduces <em>multiple</em> regression involving more than one explanatory variable. The use of categorical explanatory variables in such models is explained in Section <a href="c-regression.html#s-regression-dummies">8.6</a>. Finally, Section <a href="c-regression.html#s-regression-rest">8.7</a> gives a brief review of some further aspects of linear regression modelling which are not covered on this course.</p>
<p><em>Example: Predictors of Infant Mortality Rate</em></p>
<p>The concepts of linear regression models will be illustrated as they are introduced with a second example from the Global Civil Society data set. The response variable will now be Infant Mortality Rate (IMR). This is an illuminating outcome variable, because it is a sensitive and unquestionably important reflection of a country’s wellbeing; whatever we mean by “development”, it is difficult to disagree that high levels of it should coincide with low levels of infant mortality. We will initially consider only one explanatory variable, Net primary school enrolment ratio, referred to as “School enrolment” for short. This is defined as the percentage of all children of primary school age who are enrolled in school. Enrolment numbers and the population size are often obtained from different official sources, which sometimes leads to discrepancies. In particular, School enrolment for several countries is recorded as over 100, which is logically impossible. This is an illustration of the kinds of measurement errors often affecting variables in the social sciences. We will use the School enrolment values as recorded, even though they are known to contain some error.</p>
<p>A scatterplot of IMR vs. School enrolment is shown in Figure <a href="c-regression.html#fig:f-imr1">8.6</a>, together with the best-fitting straight line. Later we will also consider three additional explanatory variables: Control of corruption, Income inequality and Income level of the country in three categories (c.f. Example 8.1). For further reference, Table <a href="c-regression.html#tab:t-imrvars">8.2</a> shows various summary statistics for these variables. Throughout, the analyses are restricted to those 111 countries for which all of the five variables are recorded. For this reason the correlations in Table <a href="c-regression.html#tab:t-imrvars">8.2</a> differ slightly from those in Table <a href="c-regression.html#tab:t-civilsoc-r">8.1</a>, where each correlation was calculated for all the countries with non-missing values of that pair of variables.</p>
<div class="figure"><span id="fig:f-imr1"></span>
<img src="imr1.png" alt="A scatterplot of net primary school enrolment ratio vs. Infant mortality rate for countries in the Global Civil Society data set (n=111). The solid line is the best-fitting (least squares) straight line for the points." width="510" />
<p class="caption">Figure 8.6: A scatterplot of net primary school enrolment ratio vs. Infant mortality rate for countries in the Global Civil Society data set (<span class="math inline">\(n=111\)</span>). The solid line is the best-fitting (least squares) straight line for the points.</p>
</div>
<table style="width:98%;">
<caption><span id="tab:t-imrvars">Table 8.2: </span>Summary statistics for Infant Mortality Rate (IMR) and explanatory variables for it considered in the examples of Sections <a href="c-regression.html#s-regression-simple">8.3</a> and <a href="c-regression.html#s-regression-multiple">8.5</a> (<span class="math inline">\(n=111\)</span>). See Example 8.1 for further information on the variables.</caption>
<colgroup>
<col width="53%" />
<col width="7%" />
<col width="11%" />
<col width="12%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th align="right"><br />
IMR</th>
<th align="right">School enrolment</th>
<th align="right">Control of corruption</th>
<th align="right">Income inequality</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><em>Summary statistics</em></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td>Mean</td>
<td align="right">4.3</td>
<td align="right">86.1</td>
<td align="right">50.1</td>
<td align="right">40.5</td>
</tr>
<tr class="odd">
<td>std. deviation</td>
<td align="right">4.0</td>
<td align="right">16.7</td>
<td align="right">28.4</td>
<td align="right">10.2</td>
</tr>
<tr class="even">
<td>Minimum</td>
<td align="right">0.3</td>
<td align="right">30.0</td>
<td align="right">3.6</td>
<td align="right">24.4</td>
</tr>
<tr class="odd">
<td>Maximum</td>
<td align="right">15.6</td>
<td align="right">109.0</td>
<td align="right">100.0</td>
<td align="right">70.7</td>
</tr>
<tr class="even">
<td><em>Correlation matrix</em></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td>IMR</td>
<td align="right">1</td>
<td align="right">-0.75</td>
<td align="right">-0.60</td>
<td align="right">0.39</td>
</tr>
<tr class="even">
<td>School enrolment</td>
<td align="right">-0.75</td>
<td align="right">1</td>
<td align="right">0.39</td>
<td align="right">-0.27</td>
</tr>
<tr class="odd">
<td>Control of corruption</td>
<td align="right">-0.60</td>
<td align="right">0.39</td>
<td align="right">1</td>
<td align="right">-0.27</td>
</tr>
<tr class="even">
<td>Income inequality</td>
<td align="right">0.39</td>
<td align="right">-0.27</td>
<td align="right">-0.27</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td><em>Means for countries in different income categories</em></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td>Low income (<span class="math inline">\(n=41\)</span>)</td>
<td align="right">8.2</td>
<td align="right">72.1</td>
<td align="right">27.5</td>
<td align="right">41.7</td>
</tr>
<tr class="odd">
<td>Middle income (<span class="math inline">\(n=48\)</span>)</td>
<td align="right">2.8</td>
<td align="right">92.5</td>
<td align="right">50.8</td>
<td align="right">43.3</td>
</tr>
<tr class="even">
<td>High income (<span class="math inline">\(n=22\)</span>)</td>
<td align="right">0.5</td>
<td align="right">98.4</td>
<td align="right">90.7</td>
<td align="right">32.0</td>
</tr>
</tbody>
</table>
</div>
<div id="ss-regression-simple-def" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Definition of the model</h3>
<p>The simple linear regression model defined in this section is a statistical model for a continuous, interval level response variable <span class="math inline">\(Y\)</span> given a single explanatory variable <span class="math inline">\(X\)</span>, such as IMR given School enrolment. The model will be used to carry out statistical inference on the association between the variables in a population (which in the IMR example is clearly again of the conceptual variety).</p>
<p>For motivation, recall first the situation considered in Section <a href="c-means.html#s-means-inference">7.3</a>. There the data consisted of observations <span class="math inline">\((Y_{i}, X_{i})\)</span> for <span class="math inline">\(i=1,2,\dots,n\)</span>, which were assumed to be statistically independent. The response variable <span class="math inline">\(Y\)</span> was continuous but <span class="math inline">\(X\)</span> had only two possible values, coded 1 and 2. A model was then set up where the population distribution of <span class="math inline">\(Y\)</span> had mean <span class="math inline">\(\mu_{1}\)</span> and variance <span class="math inline">\(\sigma^{2}_{1}\)</span> for units with <span class="math inline">\(X=1\)</span>, and mean <span class="math inline">\(\mu_{2}\)</span> and variance <span class="math inline">\(\sigma^{2}_{2}\)</span> when <span class="math inline">\(X=2\)</span>. In some cases it was further assumed that the population distributions were both normal, and that the population variances were equal, i.e. that <span class="math inline">\(\sigma^{2}_{1}=\sigma^{2}_{2}\)</span>, with their common value denoted <span class="math inline">\(\sigma^{2}\)</span>. With these further assumptions, which will also be used here, the model for <span class="math inline">\(Y\)</span> given a dichotomous <span class="math inline">\(X\)</span> stated that (1) observations for different units <span class="math inline">\(i\)</span> were statistically independent; (2) each <span class="math inline">\(Y_{i}\)</span> was sampled at random from a population distribution which was normal with mean <span class="math inline">\(\mu_{i}\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>; and (3) <span class="math inline">\(\mu_{i}\)</span> depended on <span class="math inline">\(X_{i}\)</span> so that it was equal to <span class="math inline">\(\mu_{1}\)</span> if <span class="math inline">\(X_{i}\)</span> was 1 and <span class="math inline">\(\mu_{2}\)</span> if <span class="math inline">\(X_{i}\)</span> was 2.</p>
<p>The situation in this section is exactly the same, except that <span class="math inline">\(X\)</span> is now continuous instead of dichotomous. We will use the same basic model, but will change the specification of the conditional mean <span class="math inline">\(\mu_{i}\)</span> appropriately. In the light of the discussion in previous sections of this chapter, it is no surprise that this will be defined in such a way that it describes a linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This is done by setting <span class="math inline">\(\mu_{i}=\alpha+\beta X_{i}\)</span>, where <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are unknown population parameters. This is the equation of straight line (we will return to it in the next section). With this specification, the model for observations <span class="math inline">\((Y_{1},X_{1}), (Y_{2}, X_{2}), \dots, (Y_{n}, X_{n})\)</span> becomes</p>
<ol style="list-style-type: decimal">
<li><p>Observations for different units <span class="math inline">\(i\)</span> (<span class="math inline">\(=1,2,\dots,n\)</span>) are statistically independent.</p></li>
<li><p>Each <span class="math inline">\(Y_{i}\)</span> is normally distributed with mean <span class="math inline">\(\mu_{i}\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
<li><p>The means <span class="math inline">\(\mu_{i}\)</span> depend on <span class="math inline">\(X_{i}\)</span> through <span class="math inline">\(\mu_{i}=\alpha+\beta X_{i}\)</span>.</p></li>
</ol>
Often the model is expressed in an equivalent form where 2. and 3. are combined as
<span class="math display">\[\begin{equation}Y_{i}=\alpha+\beta X_{i} +\epsilon_{i}
\label{eq:slinmodel}\end{equation}\]</span>
<p>where each <span class="math inline">\(\epsilon_{i}\)</span> is normally distributed with mean 0 and variance <span class="math inline">\(\sigma^{2}\)</span>. The <span class="math inline">\(\epsilon_{i}\)</span> are known as <strong>error terms</strong> or <strong>population residuals</strong> (and the letter <span class="math inline">\(\epsilon\)</span> is the lower-case Greek “epsilon”). This formulation of the model clearly separates the mean of <span class="math inline">\(Y_{i}\)</span>, which traces the straight line <span class="math inline">\(\alpha+\beta X_{i}\)</span> as <span class="math inline">\(X_{i}\)</span> changes, from the variation around that line, which is described by the variability of <span class="math inline">\(\epsilon_{i}\)</span>.</p>
<p>The model defined above is known as the <strong>simple linear regression model</strong>:</p>
<ul>
<li><p><strong>Simple</strong> because it has only one explanatory variable, as opposed to <em>multiple</em> linear regression models which will have more than one.</p></li>
<li><p><strong>Linear</strong> because it specifies a linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.<a href="#fn46" class="footnoteRef" id="fnref46"><sup>46</sup></a></p></li>
<li><p><strong>Regression</strong>: This is now an established part of the name of the model, although the origins of the word are not central to the use of the model.<a href="#fn47" class="footnoteRef" id="fnref47"><sup>47</sup></a></p></li>
<li><p><strong>Model</strong>, because this is a statistical model in the sense discussed in the middle of Section <a href="c-contd.html#ss-contd-probdistrs-general">6.3.1</a>. In other words, the model is always only a simplified abstraction of the true, immeasurably complex processes which determine the values of <span class="math inline">\(Y\)</span>. Nevertheless, it is believed that a well-chosen model can be useful for explaining and predicting observed values of <span class="math inline">\(Y\)</span>. This spirit is captured by the well-known statement by the statistician George Box:<a href="#fn48" class="footnoteRef" id="fnref48"><sup>48</sup></a></p>
<blockquote>
<p><em>All models are wrong, but some are useful.</em></p>
</blockquote>
<p>A model like this has the advantage that it reduces the examination of associations in the population to estimation and inference on a small number of model parameters, in the case of the simple linear regression model just <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^{2}\)</span>.</p></li>
</ul>
<p>Of course, not all models are equally appropriate for given data, and some will be both wrong and useless. The results from a model should thus be seriously presented and interpreted only if the model is deemed to be reasonably adequate. For the simple linear regression model, this can be partly done by examining whether the scatterplot between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> appears to be reasonably consistent with a linear relationship. Some further comments on the assessment of model adequacy will be given in Section <a href="c-regression.html#s-regression-rest">8.7</a>.</p>
</div>
<div id="ss-regression-simple-int" class="section level3">
<h3><span class="header-section-number">8.3.3</span> Interpretation of the model parameters</h3>
<p>The simple linear regression model (\ref{eq:slinmodel}) has three parameters, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^{2}\)</span>. Each of these has its own interpretation, which are explained in this section. Sometimes it will be useful to illustrate the definition with specific numerical values, for which we will use ones for the model for IMR given School enrolment in our example. SPSS output for this model is shown in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>. Note that although these values are first used here to illustrate the interpretation of the <em>population</em> parameters in the model, they are of course only estimates (of a kind explained in the next section) of those parameters. Other parts of the SPSS output will be explained later in this chapter.</p>
<div class="figure"><span id="fig:f-spss-linreg"></span>
<img src="spsslinreg.png" alt="SPSS output for a simple linear regression model for Infant mortality rate given School enrolment in the Global Civil Society data." width="585" />
<p class="caption">Figure 8.7: SPSS output for a simple linear regression model for Infant mortality rate given School enrolment in the Global Civil Society data.</p>
</div>
<p>According to the model, the conditional mean (also often known as the conditional <strong>expected value</strong>) of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> in the population is (dropping the subscript <span class="math inline">\(i\)</span> for now for notational simplicity) <span class="math inline">\(\mu=\alpha+\beta X\)</span>. The two parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> in this formula are known as <strong>regression coefficients</strong>. They are interpreted as follows:</p>
<ul>
<li><p><span class="math inline">\(\alpha\)</span> is the expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> is equal to 0. It is known as the <strong>intercept</strong> or <strong>constant</strong> term of the model.</p></li>
<li><p><span class="math inline">\(\beta\)</span> is the change in the expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> increases by 1 unit. It is known as the <strong>slope</strong> term or the <strong>coefficient of</strong> <span class="math inline">\(X\)</span>.</p></li>
</ul>
<p>Just to include one mathematical proof in this coursepack, these results can be derived as follows:</p>
<ul>
<li><p>When <span class="math inline">\(X=0\)</span>, the mean of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mu=\alpha+\beta X=\alpha+\beta\times 0 =\alpha+0=\alpha\)</span>.</p></li>
<li><p>Compare two observations, one with value <span class="math inline">\(X\)</span> of the explanatory variable, and the other with one unit more, i.e. <span class="math inline">\(X+1\)</span>. The corresponding means of <span class="math inline">\(Y\)</span> are</p>
<table>
<tbody>
<tr class="odd">
<td align="right">with <span class="math inline">\(X+1\)</span>:</td>
<td align="left"><span class="math inline">\(\mu\)</span></td>
<td align="left"><span class="math inline">\(=\alpha+\beta\times (X+1)\)</span></td>
<td align="left"><span class="math inline">\(=\alpha+\beta X +\beta\)</span></td>
</tr>
<tr class="even">
<td align="right">with <span class="math inline">\(X\)</span>:</td>
<td align="left"><span class="math inline">\(\mu\)</span></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(=\alpha+\beta X\)</span></td>
</tr>
<tr class="odd">
<td align="right">Difference:</td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(\beta\)</span></td>
</tr>
</tbody>
</table></li>
</ul>
<p>which completes the proof of the claims above — Q.E.D. In case you prefer a graphical summary, this is given in Figure <a href="c-regression.html#fig:f-linmod-params">8.8</a>.</p>
<div class="figure"><span id="fig:f-linmod-params"></span>
<img src="lmparams.png" alt="Illustration of the interpretation of the regression coefficients of a simple linear regression model." width="472" />
<p class="caption">Figure 8.8: Illustration of the interpretation of the regression coefficients of a simple linear regression model.</p>
</div>
<p>The most important parameter of the model, and usually the only one really discussed in interpreting the results, is <span class="math inline">\(\beta\)</span>, the regression coefficient of <span class="math inline">\(X\)</span>. It is also called the slope because it is literally the slope of the regression line, as shown in Figure <a href="c-regression.html#fig:f-linmod-params">8.8</a>. It is the only parameter in the model which describes the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and it does so in the above terms of expected changes in <span class="math inline">\(Y\)</span> corresponding to changes in X (<span class="math inline">\(\beta\)</span> is also related to the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, in a way explained in the next section). The sign of <span class="math inline">\(\beta\)</span> indicates the direction of the association. When <span class="math inline">\(\beta\)</span> is positive (greater than 0), the regression line slopes upwards and increasing <span class="math inline">\(X\)</span> thus also increases the expected value of <span class="math inline">\(Y\)</span> — in other words, the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is positive. This is the case illustrated in Figure <a href="c-regression.html#fig:f-linmod-params">8.8</a>. If <span class="math inline">\(\beta\)</span> is negative, the regression line slopes downwards and the association is also negative. Finally, if <span class="math inline">\(\beta\)</span> is zero, the line is parallel with the <span class="math inline">\(X\)</span>-axis, so that changing <span class="math inline">\(X\)</span> does not change the expected value of <span class="math inline">\(Y\)</span>. Thus <span class="math inline">\(\beta=0\)</span> corresponds to no (linear) association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>In the real example shown in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>, <span class="math inline">\(X\)</span> is School enrolment and <span class="math inline">\(Y\)</span> is IMR. In SPSS output, the estimated regression coefficients are given in the “<strong>Coefficients</strong>” table in the column labelled “B” under “Unstandardized coefficients”. The estimated constant term <span class="math inline">\(\alpha\)</span> is given in the row labelled “(Constant)”, and the slope term on the next row, labelled with the name or label of the explanatory variable as specified in the SPSS data file — here “Net primary school enrolment ratio 2000-2001 (%)”. The value of the intercept is here 19.736 and the slope coefficient is <span class="math inline">\(-0.179\)</span>. The estimated regression line for expected IMR is thus <span class="math inline">\(19.736-0.179 X\)</span>, where <span class="math inline">\(X\)</span> denotes School enrolment. This is the line shown in Figure <a href="c-regression.html#fig:f-imr1">8.6</a>.</p>
<p>Because the slope coefficient in the example is negative, the association between the variables is also negative, i.e. higher levels of school enrolment are associated with lower levels of infant mortality. More specifically, every increase of one unit (here one percentage point) in School enrolment is associated with a decrease of 0.179 units (here percentage points) in expected IMR.</p>
<p>Since the meaning of <span class="math inline">\(\beta\)</span> is related to a unit increase of the explanatory variable, the interpretation of its magnitude depends on what those units are. In many cases one unit of <span class="math inline">\(X\)</span> is too small or too large for convenient interpretation. For example, a change of one percentage point in School enrolment is rather small, given that the range of this variable in our data is 79 percentage points (c.f. Table <a href="c-regression.html#tab:t-imrvars">8.2</a>). In such cases the results can easily be reexpressed by using multiples of <span class="math inline">\(\beta\)</span>: specifically, the effect on expected value of <span class="math inline">\(Y\)</span> of changing <span class="math inline">\(X\)</span> by <span class="math inline">\(A\)</span> units is obtained by multiplying <span class="math inline">\(\beta\)</span> by <span class="math inline">\(A\)</span>. For instance, in our example the estimated effect of increasing School enrolment by 10 percentage points is to decrease expected IMR by <span class="math inline">\(10\times 0.179=1.79\)</span> percentage points.</p>
<p>The constant term <span class="math inline">\(\alpha\)</span> is a necessary part of the model, but it is almost never of interest in itself. This is because the expected value of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X=0\)</span> is rarely specifically interesting. Very often <span class="math inline">\(X=0\)</span> is also unrealistic, as in our example where it corresponds to a country with zero primary school enrolment. There are fortunately no such countries in the data, where the lowest School enrolment is 30%. It is then of no interest to discuss expected IMR for a hypothetical country where no children went to school. Doing so would also represent unwarranted <em>extrapolation</em> of the model beyond the range of the observed data. Even though the estimated linear model seems to fit reasonably well for these data, this is no guarantee that it would do so also for countries with much lower school enrolment, even if they existed.</p>
<p>The third parameter of the simple regression model is <span class="math inline">\(\sigma^{2}\)</span>. This is the variance of the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. It is also known as the <strong>conditional variance</strong> of <span class="math inline">\(Y\)</span>, the <strong>error variance</strong> or the <strong>residual variance</strong>. Similarly, its square root <span class="math inline">\(\sigma\)</span> is known as the conditional, error or <strong>residual standard deviation</strong>. To understand <span class="math inline">\(\sigma\)</span>, let us consider a single value of <span class="math inline">\(X\)</span>, such as one corresponding to one of the vertical dashed lines in Figure <a href="c-regression.html#fig:f-linmod-params">8.8</a> or, say, school enrolment of 85 in Figure <a href="c-regression.html#fig:f-imr1">8.6</a>. The model specifies a distribution for <span class="math inline">\(Y\)</span> given any such value of <span class="math inline">\(X\)</span>. If we were to (hypothetically) collect a large number of observations, all with this same value of <span class="math inline">\(X\)</span>, the distribution of <span class="math inline">\(Y\)</span> for them would describe the conditional distribution of <span class="math inline">\(Y\)</span> given that value of <span class="math inline">\(X\)</span>. The model states that the average of these values, i.e. the conditional mean of <span class="math inline">\(Y\)</span>, is <span class="math inline">\(\alpha+\beta X\)</span>, which is the point on the regression line corresponding to <span class="math inline">\(X\)</span>. The individual values of <span class="math inline">\(Y\)</span>, however, would of course not all be on the line but somewhere around it, some above and some below.</p>
<p>The linear regression model further specifies that the form of the conditional distribution of <span class="math inline">\(Y\)</span> is approximately normal. You can try to visualise this by imagining a normal probability curve (c.f. Figure <a href="c-contd.html#fig:f-norm1">6.5</a>) on the vertical line from <span class="math inline">\(X\)</span>, centered on the regression line and sticking up from the page. The bell shape of the curve indicates that most of the values of <span class="math inline">\(Y\)</span> for a given <span class="math inline">\(X\)</span> will be close to the regression line, and only small proportions of them far from it. The residual standard deviation <span class="math inline">\(\sigma\)</span> is the standard deviation of this conditional normal distribution, in essence describing how tightly concentrated values of <span class="math inline">\(Y\)</span> tend to be around the regression line. The model assumes, mainly for simplicity, that the same value of <span class="math inline">\(\sigma\)</span> applies to the conditional distributions at all values of <span class="math inline">\(X\)</span>; this is known as the assumption of <em>homoscedasticity</em>.</p>
<p>In SPSS output, an estimate of <span class="math inline">\(\sigma\)</span> is given in the “<strong>Model Summary</strong>” table under the misleading label “Std. Error of the Estimate”. An estimate of the residual variance <span class="math inline">\(\sigma^{2}\)</span> is found also in the “<strong>ANOVA</strong>” table under “Mean Square” for “Residual”. In our example the estimate of <span class="math inline">\(\sigma\)</span> is 2.6173 (and that of <span class="math inline">\(\sigma^{2}\)</span> is 6.85). This is usually not of direct interest for interpretation, but it will be a necessary component of some parts of the analysis discussed below.</p>
</div>
<div id="ss-regression-simple-est" class="section level3">
<h3><span class="header-section-number">8.3.4</span> Estimation of the parameters</h3>
<p>Since the regression coefficients <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> and the residual standard deviation <span class="math inline">\(\sigma\)</span> are unknown population parameters, we will need to use the observed data to obtain sensible estimates for them. How to do so is now less obvious than in the cases of simple means and proportions considered before. This section explains the standard method of estimation for the parameters of linear regression models.</p>
<p>We will denote estimates of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> by <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> (“alpha-hat” and “beta-hat”) respectively (other notations are also often used, e.g. <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>). Similarly, we can define <span class="math display">\[\hat{Y}=\hat{\alpha}+\hat{\beta} X\]</span> for <span class="math inline">\(Y\)</span> given any value of <span class="math inline">\(X\)</span>. These are the values on the estimated regression line. They are known as <strong>fitted values</strong> for <span class="math inline">\(Y\)</span>, and estimating the parameters of the regression model is often referred to as “fitting the model” to the observed data. The fitted values represent our predictions of expected values of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, so they are also known as <strong>predicted values</strong> of <span class="math inline">\(Y\)</span>.</p>
<p>In particular, fitted values <span class="math inline">\(\hat{Y}_{i}=\hat{\alpha}+\hat{\beta}X_{i}\)</span> can be calculated at the values <span class="math inline">\(X_{i}\)</span> of the explanatory variable <span class="math inline">\(X\)</span> for each unit <span class="math inline">\(i\)</span> in the observed sample. These can then be compared to the correponding values <span class="math inline">\(Y_{i}\)</span> of the response variable. Their differences <span class="math inline">\(Y_{i}-\hat{Y}_{i}\)</span> are known as the (sample) <strong>residuals</strong>. These quantities are illustrated in Figure <a href="c-regression.html#fig:f-residuals">8.9</a>. This shows a fitted regression line, which is in fact the one for IMR given School enrolment also shown in Figure <a href="c-regression.html#fig:f-imr1">8.6</a>. Also shown are two points <span class="math inline">\((X_{i}, Y_{i})\)</span>. These are also from Figure <a href="c-regression.html#fig:f-imr1">8.6</a>; the rest have been omitted to simplify the plot. The point further to the left is the one for Mali, which has School enrolment <span class="math inline">\(X_{i}=43.0\)</span> and IMR <span class="math inline">\(Y_{i}=14.1\)</span>. Using the estimated coefficients <span class="math inline">\(\hat{\alpha}=19.736\)</span> and <span class="math inline">\(\hat{\beta}=-0.179\)</span> in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>, the fitted value for Mali is <span class="math inline">\(\hat{Y}_{i}=19.736-0.179\times 43.0=12.0\)</span>. Their difference is the residual <span class="math inline">\(Y_{i}-\hat{Y}_{i}=14.1-12.0=2.1\)</span>. Because the observed value is here larger than the fitted value, the residual is positive and the observed value is above the fitted line, as shown in Figure <a href="c-regression.html#fig:f-residuals">8.9</a>.</p>
<div class="figure"><span id="fig:f-residuals"></span>
<img src="lmresids.png" alt="Illustration of the quantities involved in the definitions of least squares estimates and the coefficient of determination R^{2}. See the text for explanation." width="510" />
<p class="caption">Figure 8.9: Illustration of the quantities involved in the definitions of least squares estimates and the coefficient of determination <span class="math inline">\(R^{2}\)</span>. See the text for explanation.</p>
</div>
<!--
$Y_{i}-\hat{Y}_{i}$

$Y_{i}-\hat{Y}_{i}$

$\bar{Y}$

$\hat{Y}_{i}-\bar{Y}$

$Y_{i}-\bar{Y}$

$\hat{Y}=\hat{\alpha}+\hat{\beta} X$
-->
<p>The second point shown in Figure <a href="c-regression.html#fig:f-residuals">8.9</a> corresponds to the observation for Ghana, for which <span class="math inline">\(X_{i}=58.0\)</span> and <span class="math inline">\(Y_{i}=5.7\)</span>. The fitted value is then <span class="math inline">\(\hat{Y}_{i}=19.736-0.179\times 58.0=9.4\)</span> and the residual <span class="math inline">\(Y_{i}-\hat{Y}_{i}=5.7-9.4=-3.7\)</span>. Because the observed value is now smaller than the fitted value, the residual is negative and the observed <span class="math inline">\(Y_{i}\)</span> is below the fitted regression line.</p>
So far we have still not explained how the specific values of the parameter estimates in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a> were obtained. In doing so, we are faced with the task of identifying a regression line which provides the best fit to the observed points in a scatterplot like Figure <a href="c-regression.html#fig:f-imr1">8.6</a>. Each possible choice of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> corresponds to a different regression line, and some choices are clearly better than others. For example, it seems intuitively obvious that it would be better for the line to go through the cloud of points rather than stay completely outside it. To make such considerations explicit, the residuals can be used as a criterion of model fit. The aim will then be to make the total magnitude of the residuals as small as possible, so that the fitted line is as close as possible to the observed points <span class="math inline">\(Y_{i}\)</span> in some overall sense. This cannot be done simply by adding up the residuals, because they can have different signs, and positive and negative residuals could thus cancel out each other in the addition. As often before, the way around this is to remove the signs by considering the squares of the residuals. Summing these over all units <span class="math inline">\(i\)</span> in the sample leads to the sum of squared residuals <span class="math display">\[SSE = \sum (Y_{i}-\hat{Y}_{i})^{2}.\]</span> Here <span class="math inline">\(SSE\)</span> is short for Sum of Squares of Errors (it is also often called the Residual Sum of Squares or <span class="math inline">\(RSS\)</span>). This is the quantity used as the criterion in estimating regression coefficients for a linear model. Different candidate values for <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> lead to different values of <span class="math inline">\(\hat{Y}_{i}\)</span> and thus of <span class="math inline">\(SSE\)</span>. The final estimates are the ones which give the smallest value of <span class="math inline">\(SSE\)</span>. Their formulas are
<span class="math display">\[\begin{equation}\hat{\beta}=
\frac{
\sum (X_{i}-\bar{X})(Y_{i}-\bar{Y})}
{\sum (X_{i}-\bar{X})^{2}}
=\frac{s_{xy}}{s_{x}^{2}}
\label{eq:ols-b}\end{equation}\]</span>
and
<span class="math display">\[\begin{equation}\hat{\alpha}=\bar{Y}-\hat{\beta}\bar{X}
\label{eq:ols-a}\end{equation}\]</span>
<p>where <span class="math inline">\(\bar{Y}\)</span>, <span class="math inline">\(\bar{X}\)</span>, <span class="math inline">\(s_{x}\)</span> and <span class="math inline">\(s_{xy}\)</span> are the usual sample means, standard deviations and covariances for <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. These are known as the <strong>least squares estimates</strong> of the regression coefficients (or as Ordinary Least Squares or OLS estimates), and the reasoning used to obtain them is the <strong>method of least squares</strong>.<a href="#fn49" class="footnoteRef" id="fnref49"><sup>49</sup></a> Least squares estimates are almost always used for linear regression models, and they are the ones displayed by SPSS and other software. For our model for IMR given School enrolment, the estimates are the <span class="math inline">\(\hat{\alpha}=19.736\)</span> and <span class="math inline">\(\hat{\beta}=-0.179\)</span> shown in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>.</p>
<p>The estimated coefficients can be used to calculate predicted values for <span class="math inline">\(Y\)</span> at any values of <span class="math inline">\(X\)</span>, not just those included in the observed sample. For instance, in the infant mortality example the predicted IMR for a country with School enrolment of 80% would be <span class="math inline">\(\hat{Y}=19.736-0.179\times 80=5.4\)</span>. Such predictions should usually be limited to the range of values of <span class="math inline">\(X\)</span> actually observed in the data, and extrapolation beyond these values should be avoided.</p>
The most common estimate of the remaining parameter of the model, the residual standard deviation <span class="math inline">\(\sigma\)</span>, is
<span class="math display">\[\begin{equation}\hat{\sigma}=
\sqrt{
\frac{\sum (Y_{i}-\hat{Y}_{i})^{2}}{n-(k+1)}
}
=\sqrt{
\frac{SSE}{n-(k+1)}
}
\label{eq:sigma-linreg}\end{equation}\]</span>
<p>where <span class="math inline">\(k\)</span> is here set equal to 1. This bears an obvious resemblance to the formula for the basic sample standard deviation, shown for <span class="math inline">\(Y_{i}\)</span> in (\ref{eq:sdyx}). One difference to that formula is that the denominator of (\ref{eq:sigma-linreg}) is shown as <span class="math inline">\(n-(k+1)\)</span> rather than <span class="math inline">\(n-1\)</span>. Here <span class="math inline">\(k=1\)</span> is the number of explanatory variables in the model, and <span class="math inline">\(k+1=2\)</span> is the number of regression coefficients (<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>) including the constant term <span class="math inline">\(\alpha\)</span>. The quantity <span class="math inline">\(n-(k+1)\)</span>, i.e. here <span class="math inline">\(n-2\)</span>, is the <strong>degrees of freedom</strong> (<span class="math inline">\(df\)</span>) of the parameter estimates. We will need it again in the next section. It is here given in the general form involving the symbol <span class="math inline">\(k\)</span>, so that we can later refer to the same formula for models with more explanatory variables and thus <span class="math inline">\(k\)</span> greater than 1. In SPSS output, the degrees of freedom are shown in the “<strong>ANOVA</strong>” table under “df” for “Residual”. In the infant mortality example <span class="math inline">\(n=111\)</span>, <span class="math inline">\(k=1\)</span> and <span class="math inline">\(df=111-2=109\)</span>, as shown in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>.</p>
<p>Finally, two connections between previous topics and the parameters <span class="math inline">\(\hat{\alpha}\)</span>, <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span> are worth highlighting:</p>
<ul>
<li><p>The estimated slope <span class="math inline">\(\hat{\beta}\)</span> from (\ref{eq:ols-b}) is related to the sample correlation <span class="math inline">\(r\)</span> from (\ref{eq:corr}) by <span class="math inline">\(r=(s_{x}/s_{y})\,\hat{\beta}\)</span>. In both of these it is <span class="math inline">\(\hat{\beta}\)</span> which carries information about the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The ratio <span class="math inline">\(s_{x}/s_{y}\)</span> serves only to standardise the correlation coefficient so that it is always between <span class="math inline">\(-1\)</span> and <span class="math inline">\(+1\)</span>. The slope coefficient <span class="math inline">\(\hat{\beta}\)</span> is not standardised, and the interpretation of its magnitude depends on the units of measurement of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in the way defined in Section <a href="c-regression.html#ss-regression-simple-int">8.3.3</a>.</p></li>
<li><p>Suppose we simplify the simple linear regression model (\ref{eq:slinmodel}) further by setting <span class="math inline">\(\beta=0\)</span>, thus removing <span class="math inline">\(\beta X\)</span> from the model. The new model states that all <span class="math inline">\(Y_{i}\)</span> are normally distributed with the same mean <span class="math inline">\(\alpha\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Apart from the purely notational difference of using <span class="math inline">\(\alpha\)</span> instead of <span class="math inline">\(\mu\)</span>, this is exactly the single-sample model considered in Section <a href="c-means.html#s-means-1sample">7.4</a>. Using the methods of this section to obtain estimates of the two parameters of this model also leads to exactly the same results as before. The least squares estimate of <span class="math inline">\(\alpha\)</span> is then <span class="math inline">\(\hat{\alpha}=\bar{Y}\)</span>, obtained by setting <span class="math inline">\(\hat{\beta}=0\)</span> in (\ref{eq:ols-a}). Since there is no <span class="math inline">\(\hat{\beta}\)</span> in this case, <span class="math inline">\(\hat{Y}_{i}=\bar{Y}\)</span> for all observations, <span class="math inline">\(k=0\)</span> and <span class="math inline">\(df=n-(k+1)=n-1\)</span>. Substituting these into (\ref{eq:sigma-linreg}) shows that <span class="math inline">\(\hat{\sigma}\)</span> is then equal to the usual sample standard deviation <span class="math inline">\(s_{y}\)</span> of <span class="math inline">\(Y_{i}\)</span>.</p></li>
</ul>
<div id="coefficient-of-determination-r2" class="section level4 unnumbered">
<h4>Coefficient of determination (<span class="math inline">\(R^{2}\)</span>)</h4>
<!--{#p-R2}-->
<p>The <strong>coefficient of determination</strong>, more commonly known as <span class="math inline">\(\mathbf{R^{2}}\)</span> (“R-squared”), is a measure of association very often used to describe the results of linear regression models. It is based on the same idea of sums of squared errors as least squares estimation, and on comparison of them between two models for <span class="math inline">\(Y\)</span>. The first of these models is the very simple one where the explanatory variable <span class="math inline">\(X\)</span> is not included at all. As discussed above, the estimate of the expected value of <span class="math inline">\(Y\)</span> is then the sample mean <span class="math inline">\(\bar{Y}\)</span>. This is the best prediction of <span class="math inline">\(Y\)</span> we can make, if the same predicted value is to be used for all observations. The error in the prediction of each value <span class="math inline">\(Y_{i}\)</span> in the observed data is then <span class="math inline">\(Y_{i}-\bar{Y}\)</span> (c.f. Figure <a href="c-regression.html#fig:f-residuals">8.9</a> for an illustration of this for one observation). The sum of squares of these errors is <span class="math inline">\(TSS=\sum (Y_{i}-\bar{Y})^{2}\)</span>, where <span class="math inline">\(TSS\)</span> is short for “Total Sum of Squares”. This can also be regarded as a measure of the <strong>total variation</strong> in <span class="math inline">\(Y_{i}\)</span> in the sample (note that <span class="math inline">\(TSS/(n-1)\)</span> is the usual sample variance <span class="math inline">\(s^{2}_{y}\)</span>).</p>
When an explanatory variable <span class="math inline">\(X\)</span> is included in the model, the predicted value for each <span class="math inline">\(Y_{i}\)</span> is <span class="math inline">\(\hat{Y}_{i}=\hat{\alpha}+\hat{\beta}X_{i}\)</span>, the error in this prediction is <span class="math inline">\(Y_{i}-\hat{Y}_{i}\)</span>, and the error sum of squares is <span class="math inline">\(SSE=\sum (Y_{i}-\hat{Y}_{i})^{2}\)</span>. The two sums of squares are related by
<span class="math display">\[\begin{equation}\sum (Y_{i}-\bar{Y})^{2} =\sum (Y_{i}-\hat{Y}_{i})^{2} +\sum
(\hat{Y}_{i}-\bar{Y})^{2}.
\label{eq:ss-decomp}\end{equation}\]</span>
<p>Here <span class="math inline">\(SSM=\sum (\hat{Y}_{i}-\bar{Y})^{2}=TSS-SSE\)</span> is the “Model sum of squares”. It is the reduction in squared prediction errors achieved when we make use of <span class="math inline">\(X_{i}\)</span> to predict values of <span class="math inline">\(Y_{i}\)</span> with the regression model, instead of predicting <span class="math inline">\(\bar{Y}\)</span> for all observations. In slightly informal language, <span class="math inline">\(SSM\)</span> is the part of the total variation <span class="math inline">\(TSS\)</span> “explained” by the fitted regression model. In this language, (\ref{eq:ss-decomp}) can be stated as</p>
<table style="width:98%;">
<colgroup>
<col width="29%" />
<col width="7%" />
<col width="26%" />
<col width="7%" />
<col width="27%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="center">Total variation of <span class="math inline">\(Y\)</span></td>
<td align="center">=</td>
<td align="center">Variation explained by regression</td>
<td align="center">+</td>
<td align="center">Unexplained variation</td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(TSS\)</span></td>
<td align="center"><span class="math inline">\(=\)</span></td>
<td align="center"><span class="math inline">\(SSM\)</span></td>
<td align="center"><span class="math inline">\(+\)</span></td>
<td align="center"><span class="math inline">\(SSE\)</span></td>
</tr>
</tbody>
</table>
The <span class="math inline">\(R^{2}\)</span> statistic is defined as
<span class="math display">\[\begin{equation}R^{2}= \frac{TSS-SSE}{TSS} = 1-\frac{SSE}{TSS}
=1-\frac{\sum (Y_{i}-\hat{Y}_{i})^{2}}{
\sum (Y_{i}-\bar{Y})^{2}}.
\label{eq:R2}\end{equation}\]</span>
<p>This is the <em>proportion</em> of the total variation of <span class="math inline">\(Y\)</span> in the sample explained by the fitted regression model. Its smallest possible value is 0, which is obtained when <span class="math inline">\(\hat{\beta}=0\)</span>, so that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are completely unassociated, <span class="math inline">\(X\)</span> provides no help for predicting <span class="math inline">\(Y\)</span>, and thus <span class="math inline">\(SSE=TSS\)</span>. The largest possible value of <span class="math inline">\(R^{2}\)</span> is 1, obtained when <span class="math inline">\(\hat{\sigma}=0\)</span>, so that the observed <span class="math inline">\(Y\)</span> can be predicted perfectly from the corresponding <span class="math inline">\(X\)</span> and thus <span class="math inline">\(SSE=0\)</span>. More generally, <span class="math inline">\(R^{2}\)</span> is somewhere between 0 and 1, with large values indicating strong linear association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p><span class="math inline">\(R^{2}\)</span> is clearly a Proportional Reduction of Error (PRE) measure of association of the kind discussed in Section <a href="c-descr1.html#ss-descr1-2cat-gamma">2.4.5</a>, with <span class="math inline">\(E_{1}=TSS\)</span> and <span class="math inline">\(E_{2}=SSE\)</span> in the notation of equation for the PRE measure of association in Section <a href="c-descr1.html#ss-descr1-2cat-gamma">2.4.5</a>. It is also related to the correlation coefficient. In simple linear regression, <span class="math inline">\(R^{2}\)</span> is the square of the correlation <span class="math inline">\(r\)</span> between <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(Y_{i}\)</span>. Furthermore, the square root of <span class="math inline">\(R^{2}\)</span> is the correlation between <span class="math inline">\(Y_{i}\)</span> and the fitted values <span class="math inline">\(\hat{Y}_{i}\)</span>. This quantity, known as the <strong>multiple correlation coefficient</strong> and typically denoted <span class="math inline">\(R\)</span>, is always between 0 and 1. It is equal to the correlation <span class="math inline">\(r\)</span> between <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(Y_{i}\)</span> when <span class="math inline">\(r\)</span> is positive, and the absolute value (removing the <span class="math inline">\(-\)</span> sign) of <span class="math inline">\(r\)</span> when <span class="math inline">\(r\)</span> is negative. For example, for our infant mortality model <span class="math inline">\(r=-0.753\)</span>, <span class="math inline">\(R^{2}=r^{2}=0.567\)</span> and <span class="math inline">\(R=\sqrt{R^{2}}=0.753\)</span>.</p>
<p>In SPSS output, the “<strong>ANOVA</strong>” table shows the model, error and total sums of squares <span class="math inline">\(SSM\)</span>, <span class="math inline">\(SSE\)</span> and <span class="math inline">\(TSS\)</span> in the “Sum of Squares column”, on the “Regression”, “Residual” and “Total” rows respectively. <span class="math inline">\(R^{2}\)</span> is shown in “<strong>Model summary</strong>” under “R Square” and multiple correlation <span class="math inline">\(R\)</span> next to it as “R”. Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a> shows these results for the model for IMR given School enrolment. Here <span class="math inline">\(R^{2}=0.567\)</span>. Using each country’s level of school enrolment to predict its IMR thus reduces the prediction errors by 56.7% compared to the situation where the predicted IMR is the overall sample mean (here 4.34) for every country. Another conventional way of describing this <span class="math inline">\(R^{2}\)</span> result is to say that the variation in rates of School enrolment explains 56.7% of the observed variation in Infant mortality rates.</p>
<p><span class="math inline">\(R^{2}\)</span> is a useful statistic with a convenient interpretation. However, its importance should not be exaggerated. <span class="math inline">\(R^{2}\)</span> is rarely the only or the most important part of the model results. This may be the case if the regression model is fitted solely for the purpose of <em>predicting</em> future observations of the response variable. More often, however, we are at least or more interested in examining the nature and strength of the associations between the response variable and the explanatory variable (later, variables), in which case the regression coefficients are the main parameters of interest. This point is worth emphasising because in our experience many users of linear regression models tend to place far too much importance on <span class="math inline">\(R^{2}\)</span>, often hoping to treat it as the ultimate measure of the goodness of the model. We are frequently asked questions along the lines of “My model has <span class="math inline">\(R^{2}\)</span> of 0.42 — is that good?”. The answer tends to be “I have no idea” or, at best, “It depends”. This not a sign of ignorance, because it really does depend:</p>
<ul>
<li><p>Which values of <span class="math inline">\(R^{2}\)</span> are large or small or “good” is not a statistical question but a substantive one, to which the answer depends on the nature of the variables under consideration. For example, most associations between variables in the social sciences involve much unexplained variation, so their <span class="math inline">\(R^{2}\)</span> values tend to be smaller than for quantities in, say, physics. Similarly, even in social sciences models for aggregates such as countries often have higher values of <span class="math inline">\(R^{2}\)</span> than ones for characteristics of individual people. For example, the <span class="math inline">\(R^{2}=0.567\)</span> in our infant mortality example (let alone the <span class="math inline">\(R^{2}=0.753\)</span> we will achieve for a multiple linear model for IMR in Section <a href="c-regression.html#s-regression-dummies">8.6</a>) would be unachievably high for many types of individual-level data.</p></li>
<li><p>In any case, achieving large <span class="math inline">\(R^{2}\)</span> is usually not the ultimate criterion for selecting a model, and a model can be very useful without having a large <span class="math inline">\(R^{2}\)</span>. The <span class="math inline">\(R^{2}\)</span> statistic reflects the magnitude of the variation around the fitted regression line, corresponding to the residual standard deviation <span class="math inline">\(\hat{\sigma}\)</span>. Because this is an accepted part of the model, <span class="math inline">\(R^{2}\)</span> is not a measure of how well the model fits: we can have a model which is essentially true (in that <span class="math inline">\(X\)</span> is linearly associated with <span class="math inline">\(Y\)</span>) but has large residual standard error and thus small <span class="math inline">\(R^{2}\)</span>.</p></li>
</ul>
</div>
</div>
<div id="ss-regression-simple-inf" class="section level3">
<h3><span class="header-section-number">8.3.5</span> Statistical inference for the regression coefficients</h3>
The only parameter of the simple linear regression model for which we will describe methods of statistical inference is the slope coefficient <span class="math inline">\(\beta\)</span>. Tests and confidence intervals for population values of the intercept <span class="math inline">\(\alpha\)</span> are rarely and ones about the residual standard deviation <span class="math inline">\(\sigma\)</span> almost never substantively interesting, so they will not be considered. Similarly, the only null hypothesis on <span class="math inline">\(\beta\)</span> discussed here is that its value is zero, i.e.
<span class="math display">\[\begin{equation}H_{0}:\; \beta=0.
\label{eq:H0betasimple}\end{equation}\]</span>
<p>Recall that when <span class="math inline">\(\beta\)</span> is 0, there is no linear association between the explanatory variable <span class="math inline">\(X\)</span> and the response variable <span class="math inline">\(Y\)</span>. Graphically, this corresponds to a regression line in the population which is parallel to the <span class="math inline">\(X\)</span>-axis (see plot (d) of Figure <a href="c-regression.html#fig:f-scatterplots">8.4</a> for an illustration of such a line in a sample). The hypothesis (\ref{eq:H0betasimple}) can thus be expressed in words as</p>
<span class="math display">\[\begin{equation}
H_{0}:\; \text{\emph{There is no linear association between }X \text{\emph{and }}Y\text{ \emph{in the population}}}.
\label{eq:H0betasimple2}
\end{equation}\]</span>
<p>Tests of this are usually carried out against a two-sided alternative hypothesis <span class="math inline">\(H_{a}: \; \beta\ne 0\)</span>, and we will also concentrate on this case.</p>
<p>Formulation (\ref{eq:H0betasimple2}) implies that the hypothesis that <span class="math inline">\(\beta=0\)</span> is equivalent to one that the population correlation <span class="math inline">\(\rho\)</span> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is also 0. The test statistic presented below for testing (\ref{eq:H0betasimple}) is also identical to a common test statistic for <span class="math inline">\(\rho=0\)</span>. A test of <span class="math inline">\(\beta=0\)</span> can thus be interpreted also as a test of no correlation in the population.</p>
The tests and confidence intervals involve both the estimate <span class="math inline">\(\hat{\beta}\)</span> and its estimated standard error, which we will here denote <span class="math inline">\(\hat{\text{se}}(\hat{\beta})\)</span>.<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a> It is calculated as
<span class="math display">\[\begin{equation}\hat{\text{se}}(\hat{\beta})=
\frac{\hat{\sigma}}{\sqrt{\sum\left(X_{i}-\bar{X}\right)^{2}}}
=\frac{\hat{\sigma}}{s_{x}\sqrt{n-1}}
\label{eq:sebeta}\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\sigma}\)</span> is the estimated residual standard deviation given by (\ref{eq:sigma-linreg}), and <span class="math inline">\(s_{x}\)</span> is the sample standard deviation of <span class="math inline">\(X\)</span>. The standard error indicates the level of precision with which <span class="math inline">\(\hat{\beta}\)</span> estimates the population parameter <span class="math inline">\(\beta\)</span>. The last expression in (\ref{eq:sebeta}) shows that the sample size <span class="math inline">\(n\)</span> appears in the denominator of the standard error formula. This means that the standard error becomes smaller as the sample size increases. In other words, the precision of estimation increases when the sample size increases, as with all the other estimates of population parameters we have considered before. In SPSS output, the estimated standard error is given under “Std. Error” in the “<strong>Coefficients</strong>” table. Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a> shows that <span class="math inline">\(\hat{\text{se}}(\hat{\beta})=0.015\)</span> for the estimated coefficient <span class="math inline">\(\hat{\beta}\)</span> of School enrolment.</p>
The test statistic for the null hypothesis (\ref{eq:H0betasimple}) is once again of the general form (see the beginning of Section <a href="c-probs.html#ss-probs-test1sample-teststatistic">5.5.2</a>), i.e. a point estimate divided by its standard error. Here this gives
<span class="math display">\[\begin{equation}t=\frac{\hat{\beta}}{\hat{\text{se}}(\hat{\beta})}.
\label{eq:tbeta}\end{equation}\]</span>
<p>The logic of this is the same as in previous applications of the same idea. Since the null hypothesis (\ref{eq:H0betasimple}) claims that the population <span class="math inline">\(\beta\)</span> is zero, values of its estimate <span class="math inline">\(\hat{\beta}\)</span> far from zero will be treated as evidence against the null hypothesis. What counts as “far from zero” depends on how precisely <span class="math inline">\(\beta\)</span> is estimated from the observed data by <span class="math inline">\(\hat{\beta}\)</span> (i.e. how much uncertainty there is in <span class="math inline">\(\hat{\beta}\)</span>), so <span class="math inline">\(\hat{\beta}\)</span> is standardised by dividing by its standard error to obtain the test statistic.</p>
<p>When the null hypothesis (\ref{eq:H0betasimple}) is true, the sampling distribution of the test statistic (\ref{eq:tbeta}) is a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom (i.e. <span class="math inline">\(n-(k+1)\)</span> where <span class="math inline">\(k=1\)</span> is the number of explanatory variables in the model). The <span class="math inline">\(P\)</span>-value for the test against a two-sided alternative hypothesis <span class="math inline">\(\beta\ne 0\)</span> is then the probability that a value from a <span class="math inline">\(t_{n-2}\)</span> distribution is at least as far from zero as the value of the observed test statistic. As for the tests of one and two means discussed in Chapter <a href="c-means.html#c-means">7</a>, it would again be possible to consider a large-sample version of the test which relaxes the assumption that <span class="math inline">\(Y_{i}\)</span> given <span class="math inline">\(X_{i}\)</span> are normally distributed, and uses (thanks to the Central Limit Theorem again) the standard normal distribution to obtain the <span class="math inline">\(P\)</span>-value. With linear regression models, however, the <span class="math inline">\(t\)</span> distribution version of the test is usually used and included in standard computer output, so only it will be discussed here. The difference between <span class="math inline">\(P\)</span>-values from the <span class="math inline">\(t_{n-2}\)</span> and standard normal distributions is in any case minimal when the sample size is reasonably large (at least 30, say).</p>
<p>In the infant mortality example shown in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>, the estimated coefficient of School enrolment is <span class="math inline">\(\hat{\beta}=-0.179\)</span>, and its estimated standard error is <span class="math inline">\(\hat{\text{se}}(\hat{\beta})=0.015\)</span>, so the test statistic is <span class="math display">\[t=\frac{-0.179}{0.015}=-11.94\]</span> (up to some rounding error). This is shown in the “t” column of the “<strong>Coefficients</strong>” table. The <span class="math inline">\(P\)</span>-value, obtained from the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2=109\)</span> degrees of freedom, is shown in the “Sig.” column. Here <span class="math inline">\(P&lt;0.001\)</span>, so the null hypothesis is clearly rejected. The data thus provide very strong evidence that primary school enrolment is associated with infant mortality rate in the population.</p>
In many analyses, rejecting the null hypothesis of no association will be entirely unsurprising. The question of interest is then not <em>whether</em> there is an association in the population, but <em>how strong</em> it is. This question is addressed with the point estimate <span class="math inline">\(\hat{\beta}\)</span>, combined with a confidence interval which reflects the level of uncertainty in <span class="math inline">\(\hat{\beta}\)</span> as an estimate of the population parameter <span class="math inline">\(\beta\)</span>. A confidence interval for <span class="math inline">\(\beta\)</span> with the confidence level <span class="math inline">\(1-\alpha\)</span> is given by
<span class="math display">\[\begin{equation}\hat{\beta} \pm t_{\alpha/2}^{(n-2)} \, \hat{\text{se}}(\hat{\beta})
\label{eq:cibeta}\end{equation}\]</span>
<p>where the multiplier <span class="math inline">\(t_{\alpha/2}^{(n-2)}\)</span> is obtained from the <span class="math inline">\(t_{n-2}\)</span> distribution as in previous applications of <span class="math inline">\(t\)</span>-based confidence intervals (c.f. the description in Section <a href="c-means.html#ss-means-inference-variants">7.3.4</a>). For a 95% confidence interval (i.e. one with <span class="math inline">\(\alpha=0.05\)</span>) in the infant mortality example, the multiplier is <span class="math inline">\(t_{0.025}^{(109)}=1.98\)</span>, and the endpoints of the interval are <span class="math display">\[-0.179-1.98\times 0.015=-0.209  \text{and}
-0.179+1.98\times 0.015=-0.149.\]</span> These are also shown in the last two columns of the “<strong>Coefficients</strong>” table of SPSS output. In this example we are thus 95% confident that the expected change in IMR associated with an increase of one percentage point in School enrolment is a decrease of between 0.149 and 0.209 percentage points. If you are calculating this confidence interval by hand, it is (if the sample size is at least 30) again acceptable to use the multiplier 1.96 from the standard normal distribution instead of the <span class="math inline">\(t\)</span>-based multiplier. Here this would give the confidence interval <span class="math inline">\((-0.208; -0.150)\)</span>.</p>
<p>It is often more convenient to interpret the slope coefficient in terms of larger or smaller increments in <span class="math inline">\(X\)</span> than one unit. As noted earlier, a point estimate for the effect of this is obtained by multiplying <span class="math inline">\(\hat{\beta}\)</span> by the appropriate constant. A confidence interval for it is calculated similarly, by multiplying the end points of an interval for <span class="math inline">\(\hat{\beta}\)</span> by the same constant. For example, the estimated effect of a 10-unit increase in School enrolment is <span class="math inline">\(10\times \hat{\beta}=-1.79\)</span>, and a 95% confidence interval for this is <span class="math inline">\(10\times (-0.209; -0.149)=(-2.09; -1.49)\)</span>. In other words, we are 95% confident that the effect is a decrease of between 2.09 and 1.49 percentage points.</p>

</div>
</div>
<div id="s-regression-causality" class="section level2">
<h2><span class="header-section-number">8.4</span> Interlude: Association and causality</h2>
<blockquote>
<p>Felix, qui potuit rerum cognoscere causas,<br />
atque metus omnis et inexorabile fatum<br />
subiecit pedibus strepitumque Acherontis avari</p>
<p>Blessed is he whose mind had power to probe<br />
The causes of things and trample underfoot<br />
All terrors and inexorable fate<br />
And the clamour of devouring Acheron</p>
<p>(Publius Vergilius Maro: <em>Georgica</em> (37-30 BCE), 2.490-492;<br />
translation by L. P. Wilkinson)</p>
</blockquote>
<p>These verses from Virgil’s Georgics are the source of the LSE motto — “Rerum cognoscere causas”, or “To know the causes of things” — which you can see on the School’s coat of arms on the cover of this coursepack. As the choice of the motto suggests, questions on <em>causes</em> and <em>effects</em> are of great importance in social and all other sciences. Causal connections are the mechanisms through which we try to understand and predict what we observe in the world, and the most interesting and important research questions thus tend to involve claims about causes and effects.</p>
<p>We have already discussed several examples of statistical analyses of <em>associations</em> between variables. Association is not the same as causation, as two variables can be statistically associated without being in any way directly causally related. Finding an association is thus not <em>sufficient</em> for establishing a causal link. It is, however, <em>necessary</em> for such a claim: if two variables are not associated, they will not be causally connected either. This implies that examination of associations must be a part of any analysis aiming to obtain conclusions about causal effects.</p>
<p>Definition and analysis of causal effects are considered in more detail on the course MY400 and in much greater depth still on MY457. Here we will discuss only the following simplified empirical version of the question.<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a> Suppose we are considering two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and suspect that <span class="math inline">\(X\)</span> is a cause of <span class="math inline">\(Y\)</span>. To support such a claim, we must be able to show that the following three conditions are satisfied:</p>
<ol style="list-style-type: decimal">
<li><p>There is a statistical association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p>An appropriate time order: <span class="math inline">\(X\)</span> comes before <span class="math inline">\(Y\)</span>.</p></li>
<li><p>All alternative explanations for the association are ruled out.</p></li>
</ol>
<p>The first two conditions are relatively straightforward, at least in principle. Statistical associations are examined using the kinds of techniques covered on this course, and decisions about whether or not there is an association are usually made largely with the help of statistical inference. Note also that making <em>statistical</em> associations one of the conditions implies that this empirical definition of causal effects is not limited to <em>deterministic</em> effects, where a particular value of <span class="math inline">\(X\)</span> always leads to exactly the same value of <span class="math inline">\(Y\)</span>. Instead, we consider <em>probabilistic</em> causal effects, where changes in <span class="math inline">\(X\)</span> make different values of <span class="math inline">\(Y\)</span> more or less likely. This is clearly crucial in the social sciences, where hardly any effects are even close to deterministic.</p>
<p>The second condition is trivial in many cases where <span class="math inline">\(X\)</span> must logically precede <span class="math inline">\(Y\)</span> in time: for example, a person’s sex is clearly determined before his or her income at age 20. In other cases the order is less obvious: for example, if we consider the relationship between political attitudes and readership of different newspapers, it may not be clear whether attitude came before choice of paper of vice versa. Clarifying the time order in such cases requires careful research design, often involving measurements taken at several different times.</p>
<p>The really difficult condition is the third one. The list of “all alternative explanations” is essentially endless, and we can hardly ever be sure that all of them have been “ruled out”. Most of the effort and ingenuity in research design and analysis in a study of any causal hypothesis usually goes into finding reasonably convincing ways of eliminating even the most important alternative explanations. Here we will discuss only one general class of such explanations, that of spurious associations due to common causes of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Suppose that we observe an association, here denoted symbolically by <span class="math inline">\(X\)</span> — <span class="math inline">\(Y\)</span>, and would like to claim that this implies a causal connection <span class="math inline">\(X\longrightarrow Y\)</span>. One situation where such a claim is <em>not</em> justified is when both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are caused by a third variable <span class="math inline">\(Z\)</span>, as in the graph in Figure <a href="c-regression.html#fig:f-xyzspurious">8.10</a>. If we here consider only <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, they will appear to be associated, but the connection is not a causal one. Instead, it is a <strong>spurious association</strong> induced by the dependence on both variables on the common cause <span class="math inline">\(Z\)</span>.</p>
<div class="figure"><span id="fig:f-xyzspurious"></span>
<img src="xyzspurious.png" alt="A graphical representation of a spurious association between X and Y, explained by dependence on a common cause Z." width="139" />
<p class="caption">Figure 8.10: A graphical representation of a spurious association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, explained by dependence on a common cause <span class="math inline">\(Z\)</span>.</p>
</div>
<p>To illustrate a spurious association with a silly but memorable teaching example, suppose that we examine a sample of house fires in London, and record the number of fire engines sent to each incident (<span class="math inline">\(X\)</span>) and the amount of damage caused by the fire (<span class="math inline">\(Y\)</span>). There will be a strong association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, with large numbers of fire engines associated with large amounts of damage. It is also reasonably clear that the number of fire engines is determined before the final extent of damage. The first two conditions discussed above are thus satisfied. We would, however, be unlikely to infer from this that the relationship is causal and conclude that we should try to reduce the cost of fires by dispatching fewer fire engines to them. This is because the association between the number of fire engines and the amount of damages is due to both of them being influenced by the size of the fire (<span class="math inline">\(Z\)</span>). Here this is of course obvious, but in most real research questions possible spurious associations are less easy to spot.</p>
<p>How can we then rule out spurious associations due to some background variables <span class="math inline">\(Z\)</span>? The usual approach is to try to remove the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>. This means in effect setting up comparisons between units which have different values of <span class="math inline">\(X\)</span> but the same or similar values of <span class="math inline">\(Z\)</span>. Any differences in <span class="math inline">\(Y\)</span> can then more confidently be attributed to <span class="math inline">\(X\)</span>, because they cannot be due to differences in <span class="math inline">\(Z\)</span>. This approach is known as <strong>controlling</strong> for other variables <span class="math inline">\(Z\)</span> in examining the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>The most powerful way of controlling for background variables is to conduct a <strong>randomized experiment</strong>, where the values of the explanatory variable <span class="math inline">\(X\)</span> can be set by the researcher, and are assigned at random to the units in the study. For instance, of the examples considered in Chapters <a href="c-probs.html#c-probs">5</a> and <a href="c-means.html#c-means">7</a>, Examples 5.3, 5.4 and 7.3 are randomized experiments, each with an intervention variable <span class="math inline">\(X\)</span> with two possible values (placebo or real vaccine, one of two forms of a survey question, and police officer wearing or not wearing sunglasses, respectively). The randomization assures that units with different values of <span class="math inline">\(X\)</span> are on average similar in <em>all</em> variables <span class="math inline">\(Z\)</span> which precede <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, thus ruling out the possibility of spurious associations due to such variables.</p>
<p>Randomized experiments are for practical or ethical reasons infeasible in many contexts, especially in the social sciences. We may then resort to other, less powerful research designs which help to control for some background variables. This, however, is usually only partially successful, so we may also need methods of control applied at the analysis stage of the research process. This is known as <strong>statistical control</strong>. The aim of statistical control is to estimate and test associations between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> while effectively holding the values of some other variables <span class="math inline">\(Z\)</span> constant in the analysis. When the response variable is continuous, the most common way of doing this is the method of multiple linear regression which is described in the next section. When all the variables are categorical, one simple way of achieving the control is analysis of multiway contingency tables, which is described in Chapter <a href="c-3waytables.html#c-3waytables">9</a>.</p>
</div>
<div id="s-regression-multiple" class="section level2">
<h2><span class="header-section-number">8.5</span> Multiple linear regression models</h2>
<div id="ss-regression-multiple-intro" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Introduction</h3>
<p>Simple linear regression becomes multiple linear regression when more than one explanatory variable is included in the model. How this is done is explained in Section <a href="c-regression.html#ss-regression-multiple-def">8.5.2</a> below. The definition of the model builds directly on that of the simple linear model, and most of the elements of the multiple model are either unchanged or minimally modified from the simple one. As a result, we can in Section <a href="c-regression.html#ss-regression-multiple-unchanged">8.5.3</a> cover much of the multiple linear model simply by referring back to the descriptions in Section <a href="c-regression.html#s-regression-simple">8.3</a>. One aspect of the model is, however, conceptually expanded when there are multiple explanatory variables, and requires a careful discussion. This is the meaning of the regression coefficients of the explanatory variables. The interpretation of and inference for these parameters are discussed in Section <a href="c-regression.html#ss-regression-multiple-beta">8.5.4</a>. The crucial part of this interpretation, and the main motivation for considering multiple regression models, is that it is one way of implementing the ideas of statistical control in analyses for continuous response variables.</p>
<p>The concepts are be illustrated with a further example from the Global Civil Society data set. The response variable will still be the Infant mortality rate of a country, and there will be three explanatory variables: School enrolment, Control of corruption and Income inequality as measured by the Gini index (see Example 8.1). Results for this model are shown in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>, to which we will refer throughout this section. The table is also an example of the kind of format in which results of regression models are typically reported. Presenting raw computer output such as that in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a> is normally not appropriate in formal research reports.</p>
<table style="width:98%;">
<caption><span id="tab:t-imr-m2">Table 8.3: </span>Response variable: Infant Mortality Rate (%). Results for a linear regression model for Infant mortality rate given three explanatory variables in the Global Civil Society data. <span class="math inline">\(\hat{\sigma}=2.23\)</span>; <span class="math inline">\(R^{2}=0.692\)</span>; <span class="math inline">\(n=111\)</span>; <span class="math inline">\(df=107\)</span></caption>
<colgroup>
<col width="28%" />
<col width="15%" />
<col width="11%" />
<col width="8%" />
<col width="13%" />
<col width="20%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Explanatory variable</th>
<th align="right">Coefficient</th>
<th align="right">Standard error</th>
<th align="right"><span class="math inline">\(t\)</span></th>
<th align="right"><span class="math inline">\(P\)</span>-value</th>
<th align="center">95 % Confidence interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Constant</td>
<td align="right">16.40</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">School enrolment (%)</td>
<td align="right">-0.139</td>
<td align="right">0.014</td>
<td align="right">-9.87</td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center">(-0.167; -0.111 )</td>
</tr>
<tr class="odd">
<td align="left">Control of corruption</td>
<td align="right">-0.046</td>
<td align="right">0.008</td>
<td align="right">-5.53</td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center">(-0.062; -0.029)</td>
</tr>
<tr class="even">
<td align="left">Income inequality</td>
<td align="right">0.055</td>
<td align="right">0.022</td>
<td align="right">2.50</td>
<td align="right">0.014</td>
<td align="center">(0.011; 0.098)</td>
</tr>
</tbody>
</table>
</div>
<div id="ss-regression-multiple-def" class="section level3">
<h3><span class="header-section-number">8.5.2</span> Definition of the model</h3>
<p>Having multiple explanatory variables requires a slight extension of notation. Let us denote the number of explanatory variables in the model by <span class="math inline">\(k\)</span>; in our example <span class="math inline">\(k=3\)</span>. Individual explanatory variables are then denoted with subscripts as <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{k}\)</span>, in the example for instance as <span class="math inline">\(X_{1}=\)</span> School enrolment, <span class="math inline">\(X_{2}=\)</span> Control of corruption and <span class="math inline">\(X_{3}=\)</span> Income inequality. Observations for individual units <span class="math inline">\(i\)</span> (with values <span class="math inline">\(i=1,2,\dots,n\)</span>) in the sample are indicated by a further subscript, so that <span class="math inline">\(X_{1i}, X_{2i}, \dots, X_{ki}\)</span> denote the observed values of the <span class="math inline">\(k\)</span> explanatory variables for unit <span class="math inline">\(i\)</span>.</p>
The multiple linear regression model is essentially the same as the simple linear model. The values <span class="math inline">\(Y_{i}\)</span> of the response variable in the sample are again assumed to be statistically independent, and each of them is regarded as an observation sampled from a normal distribution with mean <span class="math inline">\(\mu_{i}\)</span> and variance <span class="math inline">\(\sigma^{2}\)</span>. The crucial change is that the expected values <span class="math inline">\(\mu_{i}\)</span> now depend on the multiple explanatory variables through
<span class="math display">\[\begin{equation}\mu_{i} = \alpha +\beta_{1}X_{1i}+\beta_{2}X_{2i}+\dots+\beta_{k}X_{ki}
\label{eq:mu-multiple}\end{equation}\]</span>
where the coefficients <span class="math inline">\(\beta_{1}, \beta_{2}, \dots, \beta_{k}\)</span> of individual explanatory variables are now also identified with subscripts. As in (\ref{eq:slinmodel}) for the simple linear model, the multiple model can also be expressed in the concise form
<span class="math display">\[\begin{equation}Y_{i} = \alpha
+\beta_{1}X_{1i}+\beta_{2}X_{2i}+\dots+\beta_{k}X_{ki}+\epsilon_{i}
\label{eq:mlinmodel}\end{equation}\]</span>
<p>where the error term (population residual) <span class="math inline">\(\epsilon_{i}\)</span> is normally distributed with mean 0 and variance <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p>The expected value of <span class="math inline">\(Y\)</span> as defined in (\ref{eq:mu-multiple}) is a linear function of <span class="math inline">\(X_{1}, X_{2}, \dots, X_{k}\)</span>. If there are two explanatory variables (<span class="math inline">\(k=2\)</span>), <span class="math inline">\(\mu\)</span> is described by a flat <em>plane</em> as <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> take different values (think of a flat sheet of paper, at an angle and extended indefinitely in all directions, cutting across a room in the air). A plane is the two-dimensional generalisation of a one-dimensional straight line. The actual observations of <span class="math inline">\(Y_{i}\)</span> now correspond to points in a three-dimensional space, and they are generally not on the regression plane (think of them as a swarm of bees in the air, some perhaps sitting on that sheet of paper but most hovering above or below it). When <span class="math inline">\(k\)</span> is larger than 2, the regression surface is a higher-dimensional linear object known as a hyperplane. This is impossible to visualise in our three-dimensional world, but mathematically the idea of the model remains unchanged. In each case, the observed values of <span class="math inline">\(Y\)</span> exist in a yet higher dimension, so they cannot in general be predicted exactly even with multiple explanatory variables. A regression plane defined by several <span class="math inline">\(X\)</span>-variables does nevertheless allow for more flexibility for <span class="math inline">\(\mu_{i}\)</span> than a straight line, so it is in general possible to predict <span class="math inline">\(Y_{i}\)</span> more accurately with a multiple regression model than a simple one. This, however, is not usually the only or main criterion for selecting a good regression model, for reasons discussed in Section <a href="c-regression.html#ss-regression-multiple-beta">8.5.4</a> below.</p>
</div>
<div id="ss-regression-multiple-unchanged" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Unchanged elements from simple linear models</h3>
<p>As mentioned at the beginning of this section, most elements of the multiple linear regression model are the same or very similar as for the simple model, and require little further explanation:</p>
<ul>
<li><p>The <strong>constant term</strong> (intercept) <span class="math inline">\(\alpha\)</span> is interpreted as the expected value of <span class="math inline">\(Y\)</span> when all of the explanatory variables have the value 0. This can be seen by setting <span class="math inline">\(X_{1i}, X_{2i}, \dots, X_{ki}\)</span> all to 0 in (\ref{eq:mu-multiple}). As before, <span class="math inline">\(\alpha\)</span> is rarely of any substantive interest. In the example in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>, the estimated value of <span class="math inline">\(\alpha\)</span> is 16.40.</p></li>
<li><p>The <strong>residual standard deviation</strong> <span class="math inline">\(\sigma\)</span> is the standard deviation of the conditional distribution of <span class="math inline">\(Y\)</span> given the values of all of <span class="math inline">\(X_{1}, X_{2}, \dots, X_{k}\)</span>. It thus describes the magnitude of the variability of <span class="math inline">\(Y\)</span> around the regression plane. The model assumes that <span class="math inline">\(\sigma\)</span> is the same at all values of the explanatory variables. In Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>, the estimate of <span class="math inline">\(\sigma\)</span> is 2.23.</p></li>
<li><strong>Estimates of the regression coefficients</strong> are here denoted with hats as <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}_{1}, \hat{\beta}_{2}, \dots, \hat{\beta}_{k}\)</span>, and fitted (predicted) values for <span class="math inline">\(Y_{i}\)</span> are given by
<span class="math display">\[\begin{equation}\hat{Y}_{i}=\hat{\alpha}+\hat{\beta}_{1}X_{1i}+\hat{\beta}_{2}X_{2i}+\dots+
\hat{\beta}_{k}X_{ki}.
\label{eq:Yhat}\end{equation}\]</span>
<p>The estimated regression coefficients are again obtained with the method of <strong>least squares</strong> by finding the values for <span class="math inline">\(\hat{\alpha}, \hat{\beta}_{1}, \hat{\beta}_{2}, \dots, \hat{\beta}_{k}\)</span> which make the error sum of squares <span class="math inline">\(SSE=\sum (Y_{i}-\hat{Y}_{i})^{2}\)</span> as small as possible. This is both mathematically and intuitively the same exercise as least squares estimation for a simple linear model, except with more dimensions: here we are finding the best-fitting hyperplane through a high-dimensional cloud of points rather than the best-fitting straight line through a two-dimensional scatterplot.</p>
<p>With more than one explanatory variable, the computational formulas for the estimates become difficult to write down<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a> and essentially impossible to calculate by hand. This is not a problem in practice, as they are easily computed by statistical software such as SPSS. In Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>, the least squares estimates of the regression coefficients are shown in the “Coefficient” column. Each row of the table gives the coefficient for one explanatory variable, identified in the first column. A similar format is adopted in SPSS output, where the “<strong>Coefficients</strong>” table looks very similar to the main part of Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>. The arrangement of other parts of SPSS output for multiple linear regression is essentially unchanged from the format shown in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>.</p></li>
<li><p>Predicted values for <span class="math inline">\(Y\)</span> can be calculated from (\ref{eq:Yhat}) for any set of values of the explanatory variables (whether those observed in the data or not, as long as extrapolation outside their observed ranges is avoided). This is often very useful for illustrating the implications of a fitted model. For example, Table <a href="c-regression.html#tab:t-imrvars">8.2</a> shows that the sample averages of the explanatory variables in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a> are approximately 86 for School enrolment (<span class="math inline">\(X-{1}\)</span>), 50 for Control of corruption (<span class="math inline">\(X_{2}\)</span>) and 40 for Income inequality (<span class="math inline">\(X_{3}\)</span>). The predicted IMR for a hypothetical “average” country with all these values would be <span class="math display">\[\hat{Y}=16.4-0.139\times 86-0.046\times 50+0.055\times 40=4.35\]</span> using the estimated intercept <span class="math inline">\(\hat{\alpha}=16.4\)</span>, and the estimated regression coefficients <span class="math inline">\(\hat{\beta}_{1}=-0.139\)</span>, <span class="math inline">\(\hat{\beta}_{2}=-0.046\)</span> and <span class="math inline">\(\hat{\beta}_{3}=0.055\)</span> for <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span> and <span class="math inline">\(X_{3}\)</span>. For further illustration, we might compare this to other predicted values calculated for, say, different combinations of large and/or small values of the explanatory variables.</p></li>
<li><p>The estimated residual standard error <span class="math inline">\(\hat{\sigma}\)</span> is again calculated from (\ref{eq:sigma-linreg}), using the appropriate value of <span class="math inline">\(k\)</span>. Here <span class="math inline">\(n=111\)</span> and <span class="math inline">\(k=3\)</span>, and so the degrees of freedom are <span class="math inline">\(df=n-(k+1)=n-4=107\)</span>. The estimate is <span class="math inline">\(\hat{\sigma}=2.23\)</span>.</p></li>
<li><p>The explanation of the coefficient of determination <span class="math inline">\(R^{2}\)</span> is entirely unchanged from the one given under “Coefficient of determination (<span class="math inline">\(R^{2}\)</span>)” in Section <a href="c-regression.html#ss-regression-simple-est">8.3.4</a>. It is still calculated with the formula (\ref{eq:R2}), and its interpretation is also the same. The <span class="math inline">\(R^{2}\)</span> statistic thus describes the proportion of the sample variation in <span class="math inline">\(Y\)</span> explained by the regression model, i.e. by the variation in the explanatory variables. Similarly, the multiple correlation coefficient <span class="math inline">\(R=\sqrt{R^{2}}\)</span> is again the correlation between the observed <span class="math inline">\(Y_{i}\)</span> and the fitted values <span class="math inline">\(\hat{Y}_{i}\)</span>. In our example, <span class="math inline">\(R^{2}=0.692\)</span> (and <span class="math inline">\(R=\sqrt{0.692}=0.832\)</span>), i.e. about 69.2% of the observed variation in IMR between countries is explained by the variation in levels of School enrolment, Control of corruption and Income inequality between them. Compared to the <span class="math inline">\(R^{2}=0.567\)</span> for the simple regression model in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>, adding the two new explanatory variables has increased <span class="math inline">\(R^{2}\)</span> by 12.5 percentage points, which seems like a reasonably large increase.</p></li>
</ul>
</div>
<div id="ss-regression-multiple-beta" class="section level3">
<h3><span class="header-section-number">8.5.4</span> Interpretation and inference for the regression coefficients</h3>
<div id="interpretation" class="section level4 unnumbered">
<h4>Interpretation</h4>
<p>The concept of statistical control was outlined in Section <a href="c-regression.html#s-regression-causality">8.4</a> above. In essence, its idea is to examine the association between a response variable and a particular explanatory variable, while holding all other explanatory variables at constant values. This is useful for assessing claims about causal effects, but also more broadly whenever we want to analyse associations free of the confounding effects of other variables.</p>
<p>When all of the variables were categorical, statistical control could be carried out obviously and transparently by considering partial tables, where the control variables are literally held constant. This is not possible when some of the control variables are continuous, because they then have too many different values for it to be feasible to consider each one separately. Instead, statistical control is implemented with the help of a multiple regression model, and interpreted in terms of the regression coefficients.</p>
Consider, for example, a linear regression model with three explanatory variables <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span> and <span class="math inline">\(X_{3}\)</span>. This specifies the expected value of <span class="math inline">\(Y\)</span> as
<span class="math display">\[\begin{equation}\mu=\alpha+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}X_{3}
\label{eq:mu1}\end{equation}\]</span>
for any values of <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span> and <span class="math inline">\(X_{3}\)</span>. Suppose now that we consider a second observation, which has the same values of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> as before, but the value of <span class="math inline">\(X_{3}\)</span> larger by one unit, i.e. <span class="math inline">\(X_{3}+1\)</span>. The expected value of <span class="math inline">\(Y\)</span> is now
<span class="math display">\[\begin{equation}\mu=\alpha+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}(X_{3}+1)=
\alpha+\beta_{1}X_{1}+\beta_{2}X_{2}+\beta_{3}X_{3}+\beta_{3}.
\label{eq:mu2}\end{equation}\]</span>
<p>Subtracting (\ref{eq:mu1}) from (\ref{eq:mu2}) leaves us with <span class="math inline">\(\beta_{3}\)</span>. In other words, <span class="math inline">\(\beta_{3}\)</span> is the change in expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_{3}\)</span> is increased by one unit, while keeping the values of <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> unchanged. The same result would obviously be obtained for <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>, and for models with any number of explanatory variables. Thus in general</p>
<ul>
<li>The regression coefficient of any explanatory variable in a multiple linear regression model shows the change in expected value of the response variable <span class="math inline">\(Y\)</span> when that explanatory variable is increased by one unit, while holding all other explanatory variables constant.</li>
</ul>
<p>When there is only one explanatory variable, the “while holding…” part is omitted and the interpretation becomes the one for simple linear regression in Section <a href="c-regression.html#ss-regression-simple-int">8.3.3</a>.</p>
<p>This interpretation of the regression coefficients is obtained by “increasing by one unit” and “holding constant” values of explanatory variables by mathematical manipulations alone. It is thus true within the model even when the values of the explanatory variables are not and cannot actually be controlled and set at different values by the researcher. This, however, also implies that this appealing interpretation is a mathematical construction which does not automatically correspond to reality. In short, the interpretation of the regression coefficients is always mathematically true, but whether it is also an approximately correct description of an association in the real world depends on the appropriateness of the model for the data and study at hand. In some studies it is indeed possible to manipulate at least some explanatory variables, and corresponding regression models can then help to draw reasonably strong conclusions about associations between variables. Useful results can also be obtained in studies where no variables are in our control (so-called <em>observational studies</em>), as long as the model is selected carefully. This requires, in particular, that a linear model specification is adequate for the data, and that no crucially important explanatory variables have been omitted from the model.</p>
<p>In the IMR example, the estimated coefficients in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a> are interpreted as follows:</p>
<ul>
<li><p>Holding levels of Control of corruption and Income inequality constant, increasing School enrolment by one percentage point decreases expected IMR by 0.139 percentage points.</p></li>
<li><p>Holding levels of School enrolment and Income inequality constant, increasing Control of corruption by one unit decreases expected IMR by 0.046 percentage points.</p></li>
<li><p>Holding levels of School enrolment and Control of corruption constant, increasing Income inequality by one unit increases expected IMR by 0.055 percentage points.</p></li>
</ul>
<p>Instead of “holding constant”, we often talk about “controlling for” other variables in such statements. As before, it may be more convenient to express the interpretations in terms of other increments than one unit (e.g. ten units of the measure of Income inequality) by multiplying the coefficient by the correponding value.</p>
<p>The association between the response variable <span class="math inline">\(Y\)</span> and a particular explanatory variable <span class="math inline">\(X\)</span> described by the coefficient of <span class="math inline">\(X\)</span> in a multiple regression model is known as a <strong>partial association</strong> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, controlling for the other explanatory variables in the model. This will often differ from the association estimated from a simple regression model for <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, because of the correlations between the control variables and <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. In the infant mortality example, the estimated effect of School enrolment was qualitatively unaffected by controlling for the other variables, and decreased in magnitude from -0.179 to -0.139.</p>
</div>
<div id="inference" class="section level4 unnumbered">
<h4>Inference</h4>
Inference for the regression coefficients in a multiple linear model differs from that for the simple model in interpretation but not in execution. Let <span class="math inline">\(\hat{\beta}_{j}\)</span> denote the estimated coefficient of an explanatory variable <span class="math inline">\(X_{j}\)</span> (where <span class="math inline">\(j\)</span> may be any of <span class="math inline">\(1,2,\dots,k\)</span>), and let <span class="math inline">\(\hat{\text{se}}(\hat{\beta}_{j})\)</span> denote its estimated standard error. The standard errors cannot now be calculated by hand, but they are routinely produced by computer packages and displayed as in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>. A <span class="math inline">\(t\)</span>-test statistic for the null hypothesis discussed below is given by
<span class="math display">\[\begin{equation}t=\frac{\hat{\beta}_{j}}{\hat{\text{se}}(\hat{\beta}_{j})}.
\label{eq:tbeta-m}\end{equation}\]</span>
This is identical in form to statistic (\ref{eq:tbeta}) for the simple regression model. The corresponding null hypotheses are, however, subtly but crucially different in the two cases. In a multiple model, (\ref{eq:tbeta-m}) is a test statistic for the null hypothesis
<span class="math display">\[\begin{equation}H_{0}:\; \beta_{j}=0, \text{other regression coefficients
are unrestricted}
\label{eq:H0beta-m}\end{equation}\]</span>
<p>against the alternative hypothesis <span class="math display">\[H_{a}:\; \beta_{j}\ne0, \text{other regression coefficients
are unrestricted}.\]</span> Here the statement about “unrestricted” other parameters implies that neither hypothesis makes any claims about the values of other coefficients than <span class="math inline">\(\beta_{j}\)</span>, and these are allowed to have any values. The null hypothesis is a claim about the association between <span class="math inline">\(X_{j}\)</span> and <span class="math inline">\(Y\)</span> when the other explanatory variables are already included in the model. In other words, (\ref{eq:tbeta-m}) tests <span class="math display">\[\begin{aligned}
H_{0}:&amp; &amp; \text{There is no partial association between }
X_{j} \text{ and } Y,\\
&amp;&amp;  \text{controlling for the other explanatory
variables.}\end{aligned}\]</span></p>
<p>The sampling distribution of (\ref{eq:tbeta-m}) when the null hypothesis (\ref{eq:H0beta-m}) holds is a <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-(k+1)\)</span> degrees of freedom, where <span class="math inline">\(k\)</span> is again the number of explanatory variables in the model. The test statistic and its <span class="math inline">\(P\)</span>-value from the <span class="math inline">\(t_{n-(k+1)}\)</span> distribution are shown in standard computer output, in a form similar to Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>.</p>
<p>It is important to note two things about test results for multiple regression models, such as those in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>. First, (\ref{eq:H0beta-m}) implies that if the null hypothesis is not rejected, <span class="math inline">\(X_{j}\)</span> is not associated with <span class="math inline">\(Y\)</span>, <em>if</em> the other explanatory variables are already included in the model. We would typically react to this by removing <span class="math inline">\(X_{j}\)</span> from the model, while keeping the other variables in it. This is because of a general principle that models should usually be as simple (<em>parsimonious</em>) as possible, and not include variables which have no partial effect on the response variable. Second, the <span class="math inline">\(k\)</span> tests and <span class="math inline">\(P\)</span>-values actually refer to <span class="math inline">\(k\)</span> <em>different</em> hypotheses of the form (\ref{eq:H0beta-m}), one for each explanatory variable. This raises the question of what to do if, say, tests for two variables have large <span class="math inline">\(P\)</span>-values, suggesting that either of them could be removed from the model. The appropriate reaction is to remove one of the variables (perhaps the one with the larger <span class="math inline">\(P\)</span>-value) rather than both at once, and then see whether the other still remains nonsignificant (if so, it can then also be removed). This is part of the general area of <strong>model selection</strong>, principles and practice of which are mostly beyond the scope of this course; some further comments on it are given in Section <a href="c-regression.html#s-regression-rest">8.7</a>.</p>
<p>In the example shown in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>, the <span class="math inline">\(P\)</span>-values are small for the tests for all of the coefficients. Each of the three explanatory variables thus has a significant effect on the response even after controlling for the other two, so none of the variables should be removed from the model.</p>
A confidence interval with confidence level <span class="math inline">\(1-\alpha\)</span> for any <span class="math inline">\(\beta_{j}\)</span> is given by
<span class="math display">\[\begin{equation}\hat{\beta}_{j} \pm t_{\alpha/2}^{(n-(k+1))} \,
\hat{\text{se}}(\hat{\beta}_{j}).
\label{eq:cibeta-m}\end{equation}\]</span>
<p>This is identical in form and interpretation to the interval (\ref{eq:cibeta}) for simple regression (except that the degrees of freedom are now <span class="math inline">\(df=n-(k+1)\)</span>), so no new issues arise. The confidence intervals for the coefficients in our example (where <span class="math inline">\(df=n-4=107\)</span> and <span class="math inline">\(t_{0.025}^{(107)}=1.98\)</span>) are shown in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a>.</p>
</div>
</div>
</div>
<div id="s-regression-dummies" class="section level2">
<h2><span class="header-section-number">8.6</span> Including categorical explanatory variables</h2>
<div id="ss-regression-dummies-def" class="section level3">
<h3><span class="header-section-number">8.6.1</span> Dummy variables</h3>
<p>Our models for Infant mortality rate so far did not include some more basic characteristics of the countries than school enrolment, corruption and income inequality. In particular, it seems desirable to control for the wealth of a country, which is likely to be correlated with both a health outcome like infant mortality and the other measures of development used as explanatory variables. We will do this by adding to the model the income level of the country, classified in the Global Civil Society Yearbook into three groups as Low, Middle or High income. Here one reason for considering income as a categorical variable is obviously to obtain an illustrative example for this section. However, using a variable like income in a grouped form is also more generally common practice. It also has the advantage that it is one way of dealing with cases where the effects on <span class="math inline">\(Y\)</span> of the corresponding continuous explanatory variable may be nonlinear.</p>
<p>Summary statistics in Table <a href="c-regression.html#tab:t-imrvars">8.2</a> show that income group is associated with both IMR and the explanatory variables considered so far: countries with higher income tend to have lower levels of infant mortality, and higher school enrolment, less corruption and less income inequality than lower-income countries. It thus seems that controlling for income is potentially necessary, and may change the conclusions from the model.</p>
<p>Trying to add income level to the model confronts us with a new problem: how can a categorical explanatory variable like this be used in linear regression? This question is not limited to the current example, but is unavoidable in the social sciences. Even just the standard background variables such as sex, marital status, education level, party preference, employment status and region of residence considered in most individual-level studies are mostly categorical. Similarly, most survey data on attitudes and behaviour are collected in a categorical form, and even variables such as age or income which are originally continuous are often used in a grouped form. Categorical variables are thus ubiquitous in the social sciences, and it is essential to be able to use them also in regression models. How this is done is explained in this section, again illustrated with the infant mortality example. Section <a href="c-regression.html#ss-regression-dummies-example">8.6.2</a> then describes a different example for further illustration of the techniques.</p>
<p>The key to handling categorical explanatory variables is the use of dummy variables. A <strong>dummy variable</strong> (or <strong>indicator variable</strong>) is a variable with only two values, 0 and 1. Its value is 1 if a unit is in a particular category of a categorical variable, and 0 if it is not in that category. For example, we can define for each country the variable <span class="math display">\[D_{m}=\begin{cases}
1 &amp; \text{if Income level is ``Middle&#39;&#39;} \\
0 &amp; \text{otherwise.}
\end{cases}\]</span> This would typically be referred to as something like “dummy for middle income level”. Note that the label <span class="math inline">\(D_{m}\)</span> used here has no special significance, and was chosen simply to make it easy to remember. Dummy variables will be treated below as regular explanatory variables, and we could denote them as <span class="math inline">\(X\)</span>s just like all the others. A dummy for high income level is defined similarly as <span class="math display">\[D_{h}=\begin{cases}
1 &amp; \text{if Income level is ``High&#39;&#39;} \\
0 &amp; \text{otherwise.}
\end{cases}\]</span> The two variables <span class="math inline">\(D_{m}\)</span> and <span class="math inline">\(D_{h}\)</span> are enough to identify the income level of any country. If a country is in the middle-income group, the two dummies have the values <span class="math inline">\(D_{m}=1\)</span> and <span class="math inline">\(D_{h}=0\)</span> (as no country can be in two groups), and if it has high income, the dummies are <span class="math inline">\(D_{m}=0\)</span> and <span class="math inline">\(D_{h}=1\)</span>. For low-income countries, both <span class="math inline">\(D_{m}=0\)</span> and <span class="math inline">\(D_{h}=0\)</span>. There is thus no need to define a dummy for low income, because this category is identified by the two other dummies being both zero. The same is true in general: if a categorical variable has <span class="math inline">\(K\)</span> categories, only <span class="math inline">\(K-1\)</span> dummy variables are needed to identify the category of every unit. Note, in particular, that <em>dichotomous</em> variables with only two categories (<span class="math inline">\(K=2\)</span>) are fully identified by just one dummy variable. The category which is not given a dummy of its own is known as the <strong>reference category</strong> or <strong>baseline category</strong>. Any category can be the baseline, and this is usually chosen in a way to make interpretation (discussed below) convenient. The results of the model will be the same, whichever baseline is used.</p>
<p>Categorical variables are used as explanatory variables in regression models by including the dummy variables for them in the model. The results for this in our example are shown in Table <a href="c-regression.html#tab:t-imr-m3">8.4</a>. This requires no changes in the definition or estimation of the model, and the parameter estimates, standard errors and quantities for statistical inference are obtained exactly as before even when some of the explanatory variables are dummy variables. The only aspect which requires some further explanation is the interpretation of the coefficients of the dummy variables.</p>
<table style="width:98%;">
<caption><span id="tab:t-imr-m3">Table 8.4: </span>Response variable: Infant Mortality Rate (%). Results for a linear regression model for Infant mortality rate in the Global Civil Society data, given the three explanatory variables in Table <a href="c-regression.html#tab:t-imr-m2">8.3</a> and Income level in three groups. <span class="math inline">\(\hat{\sigma}=2.01\)</span>; <span class="math inline">\(R^{2}=0.753\)</span>; <span class="math inline">\(n=111\)</span>; <span class="math inline">\(df=105\)</span>.</caption>
<colgroup>
<col width="29%" />
<col width="15%" />
<col width="9%" />
<col width="11%" />
<col width="13%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Explanatory variable</th>
<th align="right">Coefficient</th>
<th align="right">Std.  error</th>
<th align="right"><span class="math inline">\(t\)</span></th>
<th align="right"><span class="math inline">\(P\)</span>-value</th>
<th align="center">95 % Conf.  interval</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Constant</td>
<td align="right">12.00</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">School enrolment (%)</td>
<td align="right"><span class="math inline">\(-0.091\)</span></td>
<td align="right">0.016</td>
<td align="right"><span class="math inline">\(-5.69\)</span></td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center"><span class="math inline">\((-0.123; -0.059)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Control of corruption</td>
<td align="right"><span class="math inline">\(-0.020\)</span></td>
<td align="right">0.011</td>
<td align="right"><span class="math inline">\(-1.75\)</span></td>
<td align="right"><span class="math inline">\(0.083\)</span></td>
<td align="center"><span class="math inline">\((-0.043; 0.003)\)</span></td>
</tr>
<tr class="even">
<td align="left">Income inequality</td>
<td align="right">0.080</td>
<td align="right">0.021</td>
<td align="right">3.75</td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center">(0.038; 0.122)</td>
</tr>
<tr class="odd">
<td align="left">Income level (Reference group: Low)</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">  Middle</td>
<td align="right"><span class="math inline">\(-3.210\)</span></td>
<td align="right">0.631</td>
<td align="right"><span class="math inline">\(-5.09\)</span></td>
<td align="right"><span class="math inline">\(&lt;0.001\)</span></td>
<td align="center"><span class="math inline">\((-4.461; -1.958)\)</span></td>
</tr>
<tr class="odd">
<td align="left">  High</td>
<td align="right"><span class="math inline">\(-3.296\)</span></td>
<td align="right">1.039</td>
<td align="right"><span class="math inline">\(-3.17\)</span></td>
<td align="right">0.002</td>
<td align="center"><span class="math inline">\((-5.357; -1.235)\)</span></td>
</tr>
</tbody>
</table>
<p>Recall that the regression coefficient of a continuous explanatory variable <span class="math inline">\(X\)</span> is the expected change in the response variable when <span class="math inline">\(X\)</span> is increased by one unit, holding all other explanatory variables constant. Exactly the same interpretation works for dummy variables, except that it is limited to the only one-unit increase possible for them, i.e. from 0 to 1. For example, consider two (hypothetical) countries with values 0 and 1 for the dummy <span class="math inline">\(D_{m}\)</span> for middle income, but with the same values for the three continuous explanatory variables. How about the other dummy variable <span class="math inline">\(D_{h}\)</span>, for high income? The interpretation requires that this too is held constant in the comparison. If this constant value was 1, it would not be possible for <span class="math inline">\(D_{m}\)</span> to be 1 because every country is in only one income group. Thus the only value at which <span class="math inline">\(D_{h}\)</span> can be held constant while <span class="math inline">\(D_{m}\)</span> is changed is 0, so that the comparison will be between a country with <span class="math inline">\(D_{m}=1, \, D_{h}=0\)</span> and one with <span class="math inline">\(D_{m}=0,\, D_{h}=0\)</span>, both with the same values of the other explanatory variables. In other words, the interpretation of the coefficient of <span class="math inline">\(D_{m}\)</span> refers to a comparison in expected value of <span class="math inline">\(Y\)</span> between a middle-income country and a country in the baseline category of low income, controlling for the other explanatory variables. The same applies to the coefficient of <span class="math inline">\(D_{h}\)</span>, and of dummy variables in general:</p>
<ul>
<li>The coefficient of a dummy variable for a particular level of a categorical explanatory variable is interpreted as the <em>difference</em> in the expected value of the response variable <span class="math inline">\(Y\)</span> between a unit with that level of the categorical variable and a unit in the baseline category, holding all other explanatory variables constant.</li>
</ul>
<p>Here the estimated coefficient of <span class="math inline">\(D_{m}\)</span> is <span class="math inline">\(-3.21\)</span>. In other words, comparing a middle-income country and a low-income country, both with the same levels of School enrolment, Control of corruption and Income inequality, the expected IMR is 3.21 percentage points lower in the middle-income country than in the low-income one. Similarly, a high-income country has an expected IMR 3.296 percentage points lower than a low-income one, other things being equal. The expected difference between the two non-reference levels is obtained as the difference of their coefficients (or by making one of them the reference level, as discussed below); here <span class="math inline">\(-3.296-(-3.210)=-0.086\)</span>, so a high-income country has an expected IMR 0.086 percentage points lower than a middle-income one, controlling for the other explanatory variables.</p>
<p>Predicted values are again obtained by substituting values for the explanatory variables, including appropriate zeros and ones for the dummy variables, into the estimated regression equation. For example, the predicted IMR for a country with School enrolment of 86 %, Control of corruption score of 50 and Income inequality of 40 is <span class="math display">\[\begin{aligned}
\hat{Y}&amp;=&amp;12.0-0.091\times 86-0.020\times 50+0.080\times 40-3.210\times
0-3.296\times 0\\
&amp;=&amp;
6.37 \text{for a low-income country, and }\\
\hat{Y}&amp;=&amp;12.0-0.091\times 86-0.020\times 50+0.080\times 40-3.210\times
1-3.296\times 0\\
&amp;=&amp;
6.37-3.21=3.16 \text{for a middle-income country,}\end{aligned}\]</span> with a difference of 3.21, as expected. Note how the constant term 12.0 sets the level for the baseline (low-income) group, and the coefficient <span class="math inline">\(-3.21\)</span> shows the change from that level when considering a middle-income country instead. Note also that we should again avoid unrealistic combinations of the variables in such predictions. For example, the above values would not be appropriate for high-income countries, because there are no such countries in these data with Control of corruption as low as 50.</p>
<p>The choice of the reference category does not affect the fitted model, and exactly the same results are obtained with any choice. For example, if high income is used as the reference category instead, the coefficients of the three continuous variables are unchanged from Table <a href="c-regression.html#tab:t-imr-m3">8.4</a>, and the coefficients of the dummy variables for low and middle incomes are 3.296 and 0.086 respectively. The conclusions from these are the same as above: controlling for the other explanatory variables, the difference in expected IMR is 3.296 between low and high-income, 0.086 between middle and high-income and <span class="math inline">\(3.296-0.086=3.210\)</span> between low and middle-income countries. Because the choice is arbitrary, the baseline level should be selected in whichever way is most convenient for stating the interpretation. If the categorical variable is ordinal (as it is here), it makes most sense for the baseline to be the first or last category. In other ways the dummy-variable treatment makes no distinction between nominal and ordinal categorical variables. Both are treated effectively as nominal in fitting the model, and information on any ordering of the categories is ignored.</p>
<p>Significance tests and confidence intervals are obtained for coefficients of dummy variables exactly as for any regression coefficients. Since the coefficient is in this case interpreted as an expected difference between a level of a categorical variable and the reference level, the null hypothesis of a zero coefficient is the hypothesis that there is no such difference. For example, Table <a href="c-regression.html#tab:t-imr-m3">8.4</a> shows that the coefficients of both the middle income and high income dummies are clearly significantly different from zero. This shows that, controlling for the other explanatory variables, expected infant mortality for both middle and high-income countries is different from that in low-income countries. The 95% confidence intervals in Table <a href="c-regression.html#tab:t-imr-m3">8.4</a> are intervals for this difference.</p>
<p>On the other hand, the coefficients of the two higher groups are very similar, which suggests that they may not be different from each other. This can be confirmed by fitting the same model with high income as the reference level, including dummies for low and middle groups. In this model (not shown here), the coefficient of the middle income dummy corresponds to the difference of the Middle and High groups. Its <span class="math inline">\(P\)</span>-value is 0.907 and 95% confidence interval <span class="math inline">\((-1.37; 1.54)\)</span>, so the difference is clearly not significant. This suggests that we could simplify the model further by combining the two higher groups and considering only two income groups, low vs. middle/high.</p>
<p>In cases like this where a categorical explanatory variable has more than two categories, <span class="math inline">\(t\)</span>-tests of individual coefficients are tests of hypotheses about no differences between individual categories, not the hypothesis that the variable has no effect overall. This is the hypothesis that the coefficients of the dummies for <em>all</em> of the categories are zero. This requires a slightly different test, which will not be considered here. In our example the low income category is so obviously different from the other two that it is clear that the hypothesis of no overall income effect would be rejected.</p>
<p>The main reason for including income group in the example was not to study income effects themselves (it is after all not all that surprising that infant mortality is highest in poor countries), but to control for them when examining partial associations between IMR and the other explanatory variables. These describe the estimated effects of these continuous variables when comparing countries with similar income levels. Comparing the results in Tables <a href="c-regression.html#tab:t-imr-m2">8.3</a> and <a href="c-regression.html#tab:t-imr-m3">8.4</a>, it can be seen that the effect of School enrolment remains significant and negative (with higher enrolment associated with lower mortality), although its magnitude decreases somewhat after controlling for income group. Some but not all of its estimated effect in the first model is thus explained away by the fact that income is associated with both primary school enrolment and infant mortality, with richer countries having both higher enrolment and lower mortality.</p>
<p>The effect of Income inequality also remains significant in the larger model, even with a slightly increased coefficient. Countries with larger income inequality tend to have higher levels of infant mortality, even when we compare countries with similar levels of income. The effect of Control of corruption, on the other hand, is no longer significant in Table <a href="c-regression.html#tab:t-imr-m3">8.4</a>. This variable is strongly associated with income (as seen in Table <a href="c-regression.html#tab:t-imrvars">8.2</a>), with the more corrupt countries typically being poor. Controlling for income, however, level of corruption appears to have little further effect on infant mortality. This also suggests that we might simplify the model by omitting the corruption variable.</p>
<p>One final remark on dummy variables establishes a connection to the techniques discussed in Chapter <a href="c-means.html#c-means">7</a>. There we described statistical inference for comparisons of the population means of a continuous response variable <span class="math inline">\(Y\)</span> between two groups, denoted 1 and 2. Suppose now that we fit a simple linear regression model for <span class="math inline">\(Y\)</span>, with a dummy variable for group 2 as the only explanatory variable. This gives exactly the same results as the two-sample <span class="math inline">\(t\)</span>-tests and confidence intervals (under the assumption of equal variances in the groups) in Section <a href="c-means.html#s-means-inference">7.3</a>. Related to the notation of that section, the coefficients from the model are <span class="math inline">\(\hat{\alpha}=\bar{Y}_{1}\)</span>, <span class="math inline">\(\hat{\beta}=\bar{Y}_{2}-\bar{Y}_{1}\)</span>, and <span class="math inline">\(\hat{\sigma}\)</span> from (\ref{eq:sigma-linreg}) is equal to (see equation 11 in Section <a href="c-means.html#ss-means-inference-test">7.3.2</a>). Similarly, the standard error (\ref{eq:sebeta}) is the same as <span class="math inline">\(\hat{\sigma}_{\hat{\Delta}}\)</span> in the standard error equation in Section <a href="c-means.html#ss-means-inference-test">7.3.2</a>, and the test statistic (\ref{eq:tbeta}) and confidence interval (\ref{eq:cibeta}) are identical with the t-test statistic in Section <a href="c-means.html#ss-means-inference-test">7.3.2</a> and the <span class="math inline">\(t\)</span> distribution version of the equation in Section <a href="c-means.html#ss-means-inference-ci">7.3.3</a> respectively.</p>
<p>The connection between linear regression and the two-sample <span class="math inline">\(t\)</span>-test is an illustration of how statistical methods are not a series of separate tricks for different situations, but a set of connected procedures unified by common principles. Whenever possible, methods for more complex problems have been created by extending those for simpler ones, and simple analyses are in turn special cases of more general ones. Although these connections are unfortunately often somewhat obscured by changes in language and notation, trying to understand them is very useful for effective learning of statistics.</p>
</div>
<div id="ss-regression-dummies-example" class="section level3">
<h3><span class="header-section-number">8.6.2</span> A second example</h3>
<p>Because the analysis of the models for infant mortality was presented piecemeal to accompany the introduction of different elements of linear regression, an overall picture of that example may have been difficult to discern. This section describes a different analysis in a more concise manner. It is particularly an illustration of the use of dummy variables, as most of the explanatory variables are categorical. The example concerns the relationship between minimum wage and employment, and uses data originally collected and analysed by David Card and Alan Krueger.<a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a> Most of the choices of analyses and variables considered here are based on those of Card and Krueger. Their article should be consulted for discussion and more detailed analyses.</p>
<p>A minimum wage of $5.05 per hour came into effect in the U.S. state of New Jersey on April 1 1992. This represented an increase from the previous, federally mandated minimum wage of $4.25 per hour. Conventional economic theory predicts that employers will react to such an increase by cutting their work force. Here the research hypothesis is thus that employment will be reduced among businesses affected by the new minimum wage. This can be addressed using suitable data, examining a statistical hypothesis of no association between measures of minimum wage increase and change of employment, controlling for other relevant explanatory variables.</p>
<p>Card and Krueger collected data for 410 fast food restaurants at two times, about one month before and eight months after the mininum wage increase came into effect in New Jersey. Only the 368 restaurants with known values of all the variables used in the analyses are included here. Of them, 268 were in New Jersey and had starting wages below $5.05 at the time of the first interview, so that these had to be increased to meet the new minimum wage. The remaining 100 restaurants provide a control group which was not affected by the change: 75 of them were in neighbouring eastern Pennsylvania where the minimum wage remained at $4.25, and 25 were in New Jersey but had starting wages of at least $5.05 even before the increase. The theoretical prediction is that the control group should experience a smaller negative employment change than the restaurants affected by the wage increase, i.e. employment in the control restaurants should not decrease or at least decrease less than in the affected restaurants. Card and Krueger argue that fast food restaurants provide a good population for examining the research question, because they employ many low-wage workers, generally comply with minimum wage legislation, do not receive tips which would complicate wage calculations, and are relatively easy to sample and interview.</p>
<p>The response variable considered here is the change between the two interviews in full-time equivalent employment at the restaurant, defined as the number of full-time workers (including managers) plus half the number of part-time workers. This will be called “Employment change” below. We consider two variables which indicate how the restaurant was affected by the minimum wage increase. The first is simply a dummy variable which is 1 for those New Jersey restaurants where wages needed to be raised because of the increase, and 0 for the other restaurants. These will be referred to as “Affected” and “Unaffected” restaurants respectively. The second variable is also 0 for the unaffected restaurants; for the affected ones, it is the proportion by which their previous starting wage had to be increased to meet the new minimum wage. For example, if the previous starting wage was the old minimum of $4.25, this “Wage gap” is <span class="math inline">\((5.05-4.25)/4.25=0.188\)</span>. Finally, we will also use information on the chain the restaurant belongs to (Burger King, Roy Rogers, Wendy’s or KFC) and whether it is owned by the parent company or the franchisee. These will be included in the analyses as partial control for other factors affecting the labour market, which might have had a differential impact on different types of restaurants over the study period. Summary statistics for the variables are shown in Table <a href="c-regression.html#tab:t-fastfood-descr">8.5</a>.</p>
<table style="width:97%;">
<caption><span id="tab:t-fastfood-descr">Table 8.5: </span>Summary statistics for the variables considered in the minimum wage example of Section <a href="c-regression.html#ss-regression-dummies-example">8.6.2</a>. Mean employment change: Among unaffected restaurants: <span class="math inline">\(-2.93\)</span>; Among affected restaurants: <span class="math inline">\(+0.68\)</span>.</caption>
<colgroup>
<col width="24%" />
<col width="8%" />
<col width="10%" />
<col width="17%" />
<col width="20%" />
<col width="15%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><br />
</td>
<td align="right"><br />
</td>
<td align="right"><br />
</td>
<td align="right">Minimum-</td>
<td align="center">wage variable</td>
<td align="center">Response variable:</td>
</tr>
<tr class="even">
<td align="left">Group</td>
<td align="right">%</td>
<td align="right"><span class="math inline">\((n)\)</span></td>
<td align="right">Affected % (<span class="math inline">\(n\)</span>)</td>
<td align="center">Wage gap (mean for affected restaurants)</td>
<td align="center">Employment change (mean)</td>
</tr>
<tr class="odd">
<td align="left">Overall</td>
<td align="right">100</td>
<td align="right">(368)</td>
<td align="right">72.8 (268)</td>
<td align="center">0.115</td>
<td align="center"><span class="math inline">\(-0.30\)</span></td>
</tr>
<tr class="even">
<td align="left">Ownership</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="left"> Franchisee</td>
<td align="right">64.7</td>
<td align="right">(238)</td>
<td align="right">71.8 (171)</td>
<td align="center">0.122</td>
<td align="center"><span class="math inline">\(-0.17\)</span></td>
</tr>
<tr class="even">
<td align="left"> Company</td>
<td align="right">35.3</td>
<td align="right">(130)</td>
<td align="right">74.6 (97)</td>
<td align="center">0.103</td>
<td align="center"><span class="math inline">\(-0.52\)</span></td>
</tr>
<tr class="odd">
<td align="left">Chain</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left"> Burger King</td>
<td align="right">41.0</td>
<td align="right">(151)</td>
<td align="right">73.5 (111)</td>
<td align="center">0.129</td>
<td align="center"><span class="math inline">\(+0.02\)</span></td>
</tr>
<tr class="odd">
<td align="left"> Roy Rogers</td>
<td align="right">24.7</td>
<td align="right">(91)</td>
<td align="right">72.5 (66)</td>
<td align="center">0.104</td>
<td align="center"><span class="math inline">\(-1.89\)</span></td>
</tr>
<tr class="even">
<td align="left"> Wendy’s</td>
<td align="right">13.0</td>
<td align="right">(48)</td>
<td align="right">60.4 (29)</td>
<td align="center">0.086</td>
<td align="center"><span class="math inline">\(-0.28\)</span></td>
</tr>
<tr class="odd">
<td align="left"> KFC</td>
<td align="right">21.2</td>
<td align="right">(78)</td>
<td align="right">79.5 (62)</td>
<td align="center">0.117</td>
<td align="center"><span class="math inline">\(+0.94\)</span></td>
</tr>
</tbody>
</table>
<table style="width:98%;">
<caption><span id="tab:t-fastfood-models">Table 8.6: </span>Response variable: Change in Full-time equivalent employment. Two fitted models for Employment change given exposure to minimum wage increase and control variables. See the text for further details.</caption>
<colgroup>
<col width="36%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><br />
Variable</th>
<th align="right">Model (1) Coefficient (std error)</th>
<th align="right"><br />
(<span class="math inline">\(t\)</span>) <span class="math inline">\(P\)</span>-value</th>
<th align="right">Model (2) Coefficient (std error)</th>
<th align="right"><br />
(<span class="math inline">\(t\)</span>) <span class="math inline">\(P\)</span>-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Constant</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">-2.63</td>
<td align="right"></td>
<td align="right">-1.54</td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">Affected by the increase</td>
<td align="right">3.56</td>
<td align="right">(3.50)</td>
<td align="right">—</td>
<td align="right">—</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">(1.02)</td>
<td align="right">0.001</td>
<td align="right">—</td>
<td align="right">—</td>
</tr>
<tr class="odd">
<td align="left">Wage gap</td>
<td align="right">—</td>
<td align="right">—</td>
<td align="right">15.88</td>
<td align="right">(2.63)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">—</td>
<td align="right">—</td>
<td align="right">(6.04)</td>
<td align="right">0.009</td>
</tr>
<tr class="odd">
<td align="left">Ownership (vs. Franchisee)</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="even">
<td align="left">  Company</td>
<td align="right">0.22</td>
<td align="right">(0.20)</td>
<td align="right">0.43</td>
<td align="right">(0.40)</td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="right">(1.06)</td>
<td align="right">0.84</td>
<td align="right">(1.07)</td>
<td align="right">0.69</td>
</tr>
<tr class="even">
<td align="left">Chain (vs. Burger King)</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="left">  Roy Rogers</td>
<td align="right">-2.00</td>
<td align="right">(-1.56)</td>
<td align="right">-1.84</td>
<td align="right">(-1.43)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">(1.28)</td>
<td align="right">0.12</td>
<td align="right">(1.29)</td>
<td align="right">0.15</td>
</tr>
<tr class="odd">
<td align="left">  Wendy’s</td>
<td align="right">0.15</td>
<td align="right">(0.11)</td>
<td align="right">0.36</td>
<td align="right">(0.24)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">(1.44)</td>
<td align="right">0.92</td>
<td align="right">(1.46)</td>
<td align="right">0.81</td>
</tr>
<tr class="odd">
<td align="left">  KFC</td>
<td align="right">0.64</td>
<td align="right">(0.51)</td>
<td align="right">0.81</td>
<td align="right">(0.64)</td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="right">(1.25)</td>
<td align="right">0.61</td>
<td align="right">(1.26)</td>
<td align="right">0.52</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(R^{2}\)</span></td>
<td align="right">0.046</td>
<td align="right"></td>
<td align="right">0.032</td>
<td align="right"></td>
</tr>
</tbody>
</table>
<p>Table <a href="c-regression.html#tab:t-fastfood-models">8.6</a> shows results for two linear regression models for Employment change, one using the dummy for affected restaurants and one using Wage gap as an explanatory variable. Both include the same dummy variables for ownership and chain of the restaurant. Consider first the model in column (1), which includes the dummy variable for affected restaurants. The estimated coefficient of this is 3.56, which is statistically significant (with <span class="math inline">\(t=3.50\)</span> and <span class="math inline">\(P=0.001\)</span>). This means that the estimated expected Employment change for the restaurants affected by the minimum wage increase was 3.56 full-time equivalent employees larger (in the positive direction) than for unaffected restaurants, controlling for the chain and type of ownership of the restaurant. This is the opposite of the theoretical prediction that the difference would be negative, due to the minimum wage increase leading to reductions of work force among affected restaurants but little change for the unaffected ones. In fact, the summary statistics in Table <a href="c-regression.html#tab:t-fastfood-descr">8.5</a> show (although without controlling for chain and ownership) that average employment actually <em>increased</em> in absolute terms among the affected restaurants, but decreased among the unaffected ones.</p>
<p>The coefficients of the control variables in Table <a href="c-regression.html#tab:t-fastfood-models">8.6</a> describe estimated differences between company-owned and franchisee-owned restaurants, and between Burger Kings and restaurants of other chains, controlling for the other variables in the model. All of these coefficients have high <span class="math inline">\(P\)</span>-values for both models, suggesting that the differences are small. In fact, the only one which is borderline significant, after all the other control dummies are successively removed from the model (not shown here), is that Employment change seems to have been more negative for Roy Rogers restaurants than for the rest. This side issue is not investigated in detail here. In any case, the control variables have little influence on the effect of the variable of main interest: if all the control dummies are removed from Model (1), the coefficient of the dummy variable for affected restaurants becomes 3.61 with a standard error of 1.01, little changed from the estimates in Table <a href="c-regression.html#tab:t-fastfood-models">8.6</a>. This is not entirely surprising, as the control variables are weakly associated with the variable of interest: as seen in Table <a href="c-regression.html#tab:t-fastfood-descr">8.5</a>, the proportions of affected restaurants are mostly fairly similar among restaurants of all chains and types of ownership.</p>
<p>In their article, Card and Krueger carefully explore (and confirm) the robustness of their findings by considering a series of variants of the analysis, with different choices of variables and sets of observations. This is done to try to rule out the possibility that the main conclusions are reliant on, and possibly biased by, some specific features of the data and variables in the initial analysis, such as missing data or measurement error. Such sensitivity analyses would be desirable in most social science contexts, where single definitely best form of analysis is rarely obvious. Here we will carry out a modest version of such an assessment by considering the Wage gap variable as an alternative measure of the impact of minimum wage increase, instead of a dummy for affected restaurants. This is a continuous variable, but one whose values are 0 for all unaffected restaurants and vary only among the affected ones. The logic of using Wage gap as an explanatory variable here is that Employment change could be expected to depend not only on <em>whether</em> a restaurant had to increase its starting wage to meet the new minimum wage, but also on <em>how large</em> that compulsory increase was.</p>
<p>The results for the second analysis are shown as Model (2) in Table <a href="c-regression.html#tab:t-fastfood-models">8.6</a>. The results are qualitatively the same as for Model (1), in that the coefficients of the control dummies are not significant, and that of Wage gap (which is 15.88) is significant and positive. The estimated employment change is thus again larger for affected than for unaffected restaurants, and their difference now even increases when the wage rise required from an affected restaurant increases. To compare these results more directly to Model (1), we can consider a comparison between an unaffected restaurant (with Wage gap 0) and an affected one with Wage gap equal to its mean among the affected restaurants, which is 0.115 (c.f. Table <a href="c-regression.html#tab:t-fastfood-descr">8.5</a>). The estimated difference in Employment change between them, controlling for ownership and chain, is <span class="math inline">\(0.115\times 15.88=1.83\)</span>, which is somewhat lower than the 3.56 estimated from model (1).</p>
<p>This example is also a good illustration of the limitations of the <span class="math inline">\(R^{2}\)</span> statistic. The <span class="math inline">\(R^{2}\)</span> values of 0.046 and 0.032 are very small, so over 95% of the observed variation in employment changes remains unexplained by the variation in the three explanatory variables. In other words, there are large differences in Employment change experienced even by affected or unaffected restaurants of the same chain and type of ownership. This would make <em>predicting</em> the employment change for a particular restaurant a fairly hopeless task with these explanatory variables. However, prediction is not the point here. The research question focuses on possible differences in <em>average</em> changes in employment, and finding such differences is of interest even if variation around the averages is large.</p>
<p>In summary, the analysis provides no support for the theoretical prediction that the restaurants affected by a minimum wage increase would experience a larger negative job change than control restaurants. In fact, there was a small but significant difference in the opposite direction in both models described here, and in all of the analyses considered by Card and Krueger. The authors propose a number of tentative explanations for this finding, but do not present any of them as definitive.</p>
</div>
</div>
<div id="s-regression-rest" class="section level2">
<h2><span class="header-section-number">8.7</span> Other issues in linear regression modelling</h2>
<p>The material in this chapter provides a reasonably self-contained introduction to linear regression models. However, it is not possible for a course like this to cover comprehensively all aspects of the models, so some topics have been described rather superficially and several have been omitted altogether. In this section we briefly discuss some of them. First, three previously unexplained small items in standard SPSS output are described. Second, a list of further topics in linear regression is given.</p>
<p>An example of SPSS output for linear regression models was given in Figure <a href="c-regression.html#fig:f-spss-linreg">8.7</a>. Most parts of it have been explained above, but three have not been mentioned. These can be safely ignored, because each is of minor importance in most analyses. However, it is worth giving a brief explanation so as not to leave these three as mysteries:</p>
<ul>
<li><p>“Adjusted R Square” in the “<strong>Model Summary</strong>” table is a statistic defined as <span class="math inline">\(R^{2}_{adj}=[(n-1)R^{2}-k]/(n-k-1)\)</span>. This is most relevant in situations where the main purpose of the model is prediction of future observations of <span class="math inline">\(Y\)</span>. The population value of the <span class="math inline">\(R^{2}\)</span> statistic is then a key criterion of model selection. <span class="math inline">\(R^{2}_{adj}\)</span> is a better estimate of it than standard <span class="math inline">\(R^{2}\)</span>. Unlike <span class="math inline">\(R^{2}\)</span>, <span class="math inline">\(R^{2}_{adj}\)</span> does not always increase when new explanatory variables are added to the model. As a sample statistic, <span class="math inline">\(R^{2}_{adj}\)</span> does not have the same interpretation as the proportion of variation of <span class="math inline">\(Y\)</span> explained as standard <span class="math inline">\(R^{2}\)</span>.</p></li>
<li><p>The last two columns of the “<strong>ANOVA</strong>” (Analysis of Variance) table show the test statistic and <span class="math inline">\(P\)</span>-value for the so-called <span class="math inline">\(F\)</span>-test.<a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a> The null hypothesis for this is that the regression coefficients of <em>all</em> the explanatory variables are zero, i.e. <span class="math inline">\(\beta_{1}=\beta_{2}=\dots=\beta_{k}=0\)</span>. In the case of simple regression (<span class="math inline">\(k=1\)</span>), this is equivalent to the <span class="math inline">\(t\)</span>-test for <span class="math inline">\(\beta=0\)</span>. In multiple regression, it implies that none of the explanatory variables have an effect on the response variable. In practice, this is rejected in most applications. Rejecting the hypothesis implies that at least one of the explanatory variables is associated with the response, but the test provides no help for identifying <em>which</em> of the individual partial effects are significant. The <span class="math inline">\(F\)</span>-test is thus usually largely irrelevant. More useful is an extended version of it (which is not included in the default output), which is used for hypotheses that a set of several regression coefficients (but not all of them) is zero. For example, this could be used in the example of Table <a href="c-regression.html#tab:t-imr-m3">8.4</a> to test if income level had no effect on IMR, i.e. if the coefficients of the dummies for <em>both</em> middle and high income were zero.</p></li>
<li><p>The “Standardized Coefficients/Beta” in the “<strong>Coefficients</strong>” table are defined as <span class="math inline">\((s_{xj}/s_{y})\hat{\beta}_{j}\)</span>, where <span class="math inline">\(\hat{\beta}_{j}\)</span> is the estimated coefficient of <span class="math inline">\(X_{j}\)</span>, and <span class="math inline">\(s_{xj}\)</span> and <span class="math inline">\(s_{y}\)</span> are sample standard deviations of <span class="math inline">\(X_{j}\)</span> and <span class="math inline">\(Y\)</span> respectively. This is equal to the correlation of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_{j}\)</span> when <span class="math inline">\(X_{j}\)</span> is the only explanatory variable in the model, but not otherwise. The standardized coefficient describes the expected change in <span class="math inline">\(Y\)</span> in units of its sample standard error, when <span class="math inline">\(X_{j}\)</span> is increased by one standard error <span class="math inline">\(s_{xj}\)</span>, holding other explanatory variables constant. The aim of this exercise is to obtain coefficients which are more directly comparable between different explanatory variables. Ultimately it refers to the question of <strong>relative importance</strong> of the explanatory variables, i.e. “Which of the explanatory variables in the model is the most important?” This is understandably of interest in many cases, often perhaps more so than any other aspect of the model. Unfortunately, however, relative importance is also one of the hardest questions in modelling, and one without a purely statistical solution. Despite their appealing title, standardized coefficients have problems of their own and do not provide a simple tool for judging relative importance. For example, their values depend not only on the strength of association described by <span class="math inline">\(\hat{\beta}_{j}\)</span> but also on the standard deviation <span class="math inline">\(s_{xj}\)</span>, which can be different in different samples.</p>
<p>Sensible comparisons of the magnitudes of expected changes in <span class="math inline">\(Y\)</span> in response to changes in individual explanatory variables can usually be presented even without reference to standardized coefficients, simply by using the usual coefficients <span class="math inline">\(\hat{\beta}_{j}\)</span> and carefully considering the effects of suitably chosen increments of the explanatory variables. In general, it is also worth bearing in mind that questions of relative importance are often conceptually troublesome, for example between explanatory variables with very different practical implications. For instance, suppose that we have fitted a model for a measure of the health status of a person, given the amount of physical exercise the person takes (which can be changed by him/herself), investment in preventive healthcare in the area where the person lives (which can be changed, but with more effort and not by the individual) and the person’s age (which cannot be manipulated at all). The values of the unstandardized or standardized coefficients of these explanatory variables can certainly be compared, but it is not clear what statements about the relative sizes of the effects of “increasing” them would really mean.</p></li>
</ul>
<p>A further course on linear regression (e.g. first half of MY452) will typically examine the topics covered on this course in more detail, and then go on to discuss further issues. Here we will give just a list of some such topics, in no particular order:</p>
<ul>
<li><p>Model <strong>diagnostics</strong> to examine whether a particular model appears to be adequate for the data. The residuals <span class="math inline">\(Y_{i}-\hat{Y}_{i}\)</span> are a key tool in this, and the most important graphical diagnostic is simply a scatterplot of the residuals against the fitted values <span class="math inline">\(\hat{Y}_{i}\)</span>. One task of diagnostics is to identify individual <strong>outliers</strong> and <strong>influential observations</strong> which have a substantial impact on the fitted model.</p></li>
<li><p>Modelling <strong>nonlinear effects</strong> of the explanatory variables. This is mostly done simply by including transformed values like squares <span class="math inline">\(X^{2}\)</span> or logarithms <span class="math inline">\(\log(X)\)</span> as explanatory variables in the model. It is sometimes also useful to transform the response variable, e.g. using <span class="math inline">\(\log(Y)\)</span> as the response instead of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Including <strong>interactions</strong> between explanatory variables in the model. This is achieved simply by including products of them as explanatory variables.</p></li>
<li><p>Identifying and dealing with problems caused by extremely high correlations between the explanatory variables, known as problems of <strong>multicollinearity</strong>.</p></li>
<li><p><strong>Model selection</strong> to identify the best sets of explanatory variables to be used in the model. This may employ both significance tests and other approaches.</p></li>
<li><p>Analysis of Variance (<strong>ANOVA</strong>) and Analysis of Covariance (<strong>ANCOVA</strong>), which are terms used for models involving only or mostly categorical explanatory variables, particularly in the context of randomized experiments. Many of these models can be fitted as standard linear regression models with appropriate use of dummy variables, but the conventional terminology and notation for ANOVA and ANCOVA are somewhat different from the ones used here.</p></li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="39">
<li id="fn39"><p>Anheier, H., Glasius, M. and Kaldor, M. (eds.) (2005). <em>Global Civil Society 2004/5</em>. London: Sage. The book gives detailed references to the indices considered here. Many thanks to Sally Stares for providing the data in an electronic form.<a href="c-regression.html#fnref39">↩</a></p></li>
<li id="fn40"><p>Accessible at <code>data.giss.nasa.gov/gistemp/</code>. The temperatures used here are those listed in the data base under “after combining sources at same location”.<a href="c-regression.html#fnref40">↩</a></p></li>
<li id="fn41"><p>More specifically, the differences are between 11-year <em>moving averages</em>, where each year is represented by the average of the temperature for that year and the five years before and five after it (except at the ends of the series, where fewer observations are used). This is done to smooth out short-term fluctuations from the data, so that longer-term trends become more clearly visible.<a href="c-regression.html#fnref41">↩</a></p></li>
<li id="fn42"><p>This discussion is obviously rather approximate. Strictly speaking, the conditional distribution of <span class="math inline">\(Y\)</span> given, say, <span class="math inline">\(X=65\)</span> refers only to units with <span class="math inline">\(X\)</span> exactly rather than approximately equal to 65. This, however, is difficult to illustrate using a sample, because most values of a continuous <span class="math inline">\(X\)</span> appear at most once in a sample. For reasons discussed later in this chapter, the present approximate treatment still provides a reasonable general idea of the nature of the kinds of associations considered here.<a href="c-regression.html#fnref42">↩</a></p></li>
<li id="fn43"><p>This wording is commonly used for convenience even in cases where the nature of <span class="math inline">\(X\)</span> is such that its values can never actually be manipulated.<a href="c-regression.html#fnref43">↩</a></p></li>
<li id="fn44"><p>In this particular example, a more closely linear association is obtained by considering the logarithm of GDP as the response variable instead of GDP itself. This approach, which is common in dealing with skewed variables such as income, is, however, beyond the scope of this course.<a href="c-regression.html#fnref44">↩</a></p></li>
<li id="fn45"><p>Galton, F. (1888). “Co-relations and their measurement, chiefly from anthropometric data”. <em>Proceedings of the Royal Society of London</em>, <strong>45</strong>, 135–145.<a href="c-regression.html#fnref45">↩</a></p></li>
<li id="fn46"><p>This is slightly misleading: what actually matters in general is that the conditional mean is a linear function of the <em>parameters</em> <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. This need not concern us at this stage.<a href="c-regression.html#fnref46">↩</a></p></li>
<li id="fn47"><p>Galton, F. (1886). “Regression towards mediocrity in hereditary stature”. <em>Journal of the Anthropological Institute</em>, <strong>15</strong>, 246–263. The original context is essentially the one discussed on courses on research design as “regression toward the mean”.<a href="c-regression.html#fnref47">↩</a></p></li>
<li id="fn48"><p>This exact phrase apparently first appears in Box, G.E.P. (1979). Robustness in the strategy of scientific model building. In Launer, R.L. and Wilkinson, G.N., <em>Robustness in Statistics</em>, pp. 201–236.<a href="c-regression.html#fnref48">↩</a></p></li>
<li id="fn49"><p>This is another old idea. Different approaches to the problem of fitting curves to observations were gradually developed by Tobias Mayer, Rudjer Bošković and Pierre Simon Laplace from the 1750s onwards, and the method of least squares itself was presented by Adrien Marie Legendre in 1805.<a href="c-regression.html#fnref49">↩</a></p></li>
<li id="fn50"><p>It would have been more consistent with related notation used in Chapter <a href="c-means.html#c-means">7</a> to denote it something like <span class="math inline">\(\hat{\sigma}_{\hat{\beta}}\)</span>, but this would later become somewhat cumbersome.<a href="c-regression.html#fnref50">↩</a></p></li>
<li id="fn51"><p>Here adapted from a discussion in Agresti and Finlay, <em>Statistical Methods for the Social Sciences</em> (1997).<a href="c-regression.html#fnref51">↩</a></p></li>
<li id="fn52"><p>At least until we adopt extended, so-called matrix notation. In this, the least squares estimates are expressible simply as <span class="math inline">\(\hat{\boldsymbol{\beta}}= (\mathbf{X}&#39;\mathbf{X})^{-1}(\mathbf{X}&#39;\mathbf{Y})\)</span>.<a href="c-regression.html#fnref52">↩</a></p></li>
<li id="fn53"><p>Card, D. and Krueger, A. B. (1994). Minimum wages and employment: A case study of the fast-food industry in New Jersey and Pennsylvania. <em>The American Economic Review</em> <strong>84</strong>, 772–793.<a href="c-regression.html#fnref53">↩</a></p></li>
<li id="fn54"><p>The sampling distribution of this test is the <span class="math inline">\(F\)</span> distribution. The letter in both refers to Sir Ronald Fisher, the founder of modern statistics.<a href="c-regression.html#fnref54">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="c-means.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="c-3waytables.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/kbenoit/coursepack-bookdown/edit/master/08-MY451-regression.Rmd",
"text": null
},
"download": ["Coursepack-MY451.pdf", "Coursepack-MY451.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
